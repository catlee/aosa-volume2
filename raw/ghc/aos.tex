\documentclass{article}

\usepackage{code}

\title{The Glasgow Haskell Compiler}
\author{Simon Marlow \and Simon Peyton Jones}

% -----------------------------------------------------------------------------
% Points of interest to get in somewhere:
% 
%  *** Demise of the registerised C backend (war stories?)
% 
%  *** Recompilation checking, fingerprints, ABI hashes?
% 

\begin{document}

\maketitle

\makeatactive
\section{Introduction}

The Glasgow Haskell compiler started as part of an academic research
project funded by the UK government at the beginning of the 1990's,
with several goals in mind:

\begin{itemize}

\item To make freely available a robust and portable compiler for
    Haskell that generates good quality code;

\item To provide a modular foundation that other researchers can
  extend and develop;

\item To learn what real programs do.

\end{itemize}

GHC is now over 20 years old, and has been under continuous active
development since its inception.  Today, GHC releases are downloaded
by hundreds of thousands of people, the online repository of Haskell
libraries has over 3000 packages, GHC is used to teach Haskell in
many undergraduate courses, and there are a growing number of
instances of Haskell being depended upon commercially.

Over its lifetime GHC has generally had around two or three active
developers, although the number of people who have contributed some
code to GHC is in the hundreds.  While the ultimate goal for us, the
main developers of GHC, is to produce research rather than code, we
consider developing GHC to be an essential prerequisite: the artifacts
of research are fed back into GHC, so that GHC can then be used as the
basis for further research that builds on these previous ideas.
Moreover, it is important that GHC is an industrial-strength product,
since this gives greater credence to research results produced with
it.  So while GHC is stuffed full of cutting-edge research ideas, a
great deal of effort is put into ensuring that it can be relied on for
real-world use.  There has often been some tension between these two
seemingly contradictory goals, but by and large we have found a path
that is satisfactory both from the research and the real-world-use
angles.

In this chapter we want to give an overview of the architecture of
GHC, and focus on a handful of the key ideas that have been successful
in GHC (and a few that haven't).  Hopefully throughout the following
pages you will gain some insight into how we managed to keep a large
software project active for over 20 years without it collapsing under
its own weight, with what is generally considered to be a very small
development team.

% Main points we want to get across:
% 
%  *** Strong types and purity are important tools for us, they let us
%      constantly refactor and keep the code fresh.
% 
%  *** Haskell is very large, but Core is very small.  All our
%      heavyweight optimisation is done on Core.  Core passes are
%      modular - few or no dependencies between them, we can reorder and
%      repeat as we see fit.
% 
%  *** Typed intermediate language is a big win - but no static-checking,
%      just Core Lint.  But Core Lint checks more than just types.
% 

% How have we managed the growing complexity?
%
%  - wiki commentary: high-level implementation design docs
%  - good build docs on the wiki
%  - well-managed ticket system (metrics? number of open tickets
%    essentially stable, despite growing ticket submission rate)
%  - "Note" convention
%  - Large regression test suite (give metrics)
%  - nightly builds
%  - Validate protocol

\subsection{Code metrics}

The last time we measured the number of lines in GHC was in
1992\footnote{``The Glasgow Haskell compiler: a technical overview'',
  JFIT technical conference digest, 1992}, so it is interesting to
look at how things have changed since then.  Table ~\ref{t:lines}
gives a breakdown of the number of lines of code in GHC divided up
into the major components, comparing the current tallies with those
from 1992.

There are some notable aspects of these results:

\begin{itemize}
\item Despite nearly 20 years of non-stop development GHC has only
  increased in size by a factor of 5, from around 28,000 to around
  140,000 lines.  We obsessively refactor while adding new code,
  keeping the code base as fresh as possible.

\item There are several new components, although these only account
  for about 28,000 new lines.  Much of the new components are
  concerned with code generation: native code generators for various
  processors, and an LLVM code generator.  The infrastructure for the
  interactive interpreter GHCi also added over 7,000 lines.

\item The biggest increase in a single component is the type checker,
  where over 20,000 lines were added.  This is unsurprising given that
  a great deal of the recent research using GHC has been into new type
  system extensions (for example GADTs and Type Families).

\item A lot of code has been added to the @Main@ component: this is
  partly because there was previously a 3000-line Perl script
  called the ``driver'' that was rewritten in Haskell and moved into
  GHC proper, and also because support for compiling multiple modules
  was added.

\item These metrics don't take into account the runtime system, which
  itself is currently around 44,000 non-comment lines of C and C--
  (unfortunately we don't have figures for the runtime system in
  1992).  The runtime system was completely rewritten around 1997.

\item GHC has a complex build system, which today comprises about
  6,000 lines of GNU make code.  It is on its fourth complete rewrite,
  the latest being about two years ago, and each successive iteration
  has reduced the amount of code.
\end{itemize}

\begin{figure}
\begin{tabular}{|lrrr|}
\hline
Module & Lines (1992) & Lines (2011) & Increase \\

\hline
\multicolumn{4}{|l|}{\emph{Major components}} \\
Main    & 997 & 11,150 & 11.2 \\
Parser  & 1,055 & 4,098 & 3.9 \\
Renamer & 2,828 & 4,630 & 1.6 \\
Type checking & 3,352 & 24,097 & 7.2 \\
Desugaring & 1,381 & 7,091 & 5.1 \\
Core tranformations & 1,631 & 9,480 & 5.8 \\
STG transformations & 814 & 840 & 1 \\
Data-Parallel Haskell & --- & 3,718 & --- \\
Code generation & 2913 & 11,003 & 3.8 \\
Native code generation & --- & 14,138 & --- \\
LLVM code generation & --- & 2,266 & --- \\
GHCi & --- & 7,474 & --- \\

\hline
\multicolumn{4}{|l|}{\emph{Data type definitions and manipulation}} \\
Haskell abstract syntax & 2,546 & 3,700 & 1.5 \\
Core language & 1,075 & 4,798 & 4.5 \\
STG language & 517 & 693 & 1.3 \\
C{-}{-} (was Abstract C)  & 1,416 & 7,591 & 5.4 \\
Identifier representations & 1,831 & 3,120 & 1.7 \\
Type representations & 1,628 & 3,808 & 2.3 \\
Prelude definitions & 3,111 & 2,692 & 0.9 \\

\hline
\multicolumn{4}{|l|}{\emph{Utility modules}} \\
Utilities & 1,989 & 7,878 & 3.96 \\
Profiling & 191 & 367 & 1.92 \\
\hline
Total  & 28,275 & 139,955 & 4.9 \\
\hline
\label{t:lines}
\end{tabular}
\caption{Lines of code in GHC, past and present}
\end{figure}

\section{High-level structure}

At the highest level, GHC can be divided into three distinct chunks:

\begin{itemize}
\item The compiler itself.  This is essentially a Haskell program
  whose job is to convert Haskell source code into executable machine
  code.  The compiler is where most of the interesting architectural
  aspects of the design of GHC lie, and it is where we will spend the
  majority of this chapter exploring.

\item The Boot Libraries.  GHC comes with a set of libraries that we
  call the boot libraries, because they constitute precisely the
  libraries that the compiler itself depends on.  Having these
  libraries in the source tree means that GHC can bootstrap itself.
  Some of these libraries are very tightly-coupled to GHC, because
  they implement low-level functionality such as the @Int@ type in
  terms of primitives defined by the compiler and runtime system.
  Other libraries are more high-level and compiler-independent, such
  as the @Data.Map@ library.  We'll talk more about the libraries in
  Section~\ref{s:libs}.

\item The Runtime System (hereafter referred to as the RTS).  This is
  a large library of C code that handles all the tasks associated with
  \emph{running} the compiled Haskell code, including memory
  management and thread scheduling.  The RTS is linked into every
  compiled Haskell program.  The RTS represents a significant chunk of
  the development effort put into GHC, and the design decisions made
  there are responsible for some of Haskell's key strengths, such as
  its efficient support for concurrency and parallelism.  We'll
  describe the RTS in more detail in Section~\ref{s:rts}.
\end{itemize}

In fact, these three divisions correspond exactly to three
subdirectories of a GHC source tree: @compiler@, @libraries@, and
@rts@ respectively.

\subsection{The compiler}

We can divide the compiler into three:

\begin{itemize}
\item The \emph{compilation manager}, which is responsible for the
  compilation of multiple Haskell source files.  The job of the
  compilation manager is to figure out in which order to compile the
  different files, and to decide which modules do not need to be
  recompiled because none of their dependencies have changed since the
  last time they were compiled.

\item The \emph{Haskell compiler} (we abbreviate this as @Hsc@ inside
  GHC), which handles the compilation of a single Haskell source file.
  As you might imagine, most of the action happens in here.  The
  output of @Hsc@ will depend on what backend is selected: assembly,
  LLVM code, or bytecode.

\item The \emph{pipeline}, which is responsible for composing together
  any necessary external programs with @Hsc@ to compile a Haskell
  source file to object code.  For example, a Haskell source file may
  need preprocessing with the C preprocessor before feeding to @Hsc@,
  and the output of @Hsc@ is usually an assembly file that must be fed
  into the assembler to create an object file.
\end{itemize}

The compiler is not simply an executable that performs these
functions; it is itself a \emph{library} with a large API that can be
used to build not only compilers but other tools that work with
Haskell source code, such as IDEs.  More about this later in
Section~\ref{s:ghcapi}.

\subsection{Compiling Haskell code}

As with most compilers, compiling a Haskell source file proceeds in a
sequence of phases, with the output of each phase becoming the input
of the subsequent phase.  We describe the main phases below, at the
same time giving the types of the input and output data structures.
We'll describe these data structures in more detail in
Section~\ref{s:data}.

\subsubsection{Parsing}

We start in the traditional way with parsing, which takes as input a
Haskell source file and produces as output abstract syntax: @HsSyn RdrName@.
There is nothing much out of the ordinary here; GHC uses the tools @Alex@ and
@Happy@ to generate its lexical analysis and parsing code
respectively, which are analogous to the tools @lex@ and @yacc@ for C.

GHC's parser is purely functional.  In fact, the API of the GHC
library provides a pure function called @parser@ that takes a @String@
(and a few other things) and returns the parsed abstract syntax or an
error message.

\subsubsection{Renaming}

Renaming is the process of resolving all of the itentifiers in the
Haskell source code into fully-qualified names, at the same time
identifying any out-of-scope identifiers and flagging errors
appropriately.

In Haskell when you import an identifier @f@ from a module @M@, the
module @M@ may not itself have defined @f@, it may have been defined
by some other module entirely.  That information will be recorded in
the \emph{interface} for the module @M@, and it is the job of the
renamer to resolve references to @f@ to point to the module that
originally defined @f@, by using the interfaces of the modules that
are imported.  The point is that when we come to generate code later
on, we can generate a reference to the correct @f@.

The Haskell module system provides namespace management only; it is
implemented entirely by the renaming phase of compilation.  Renaming
takes Haskell abstract syntax (@HsSyn RdrName@) as input, and also
produces abstract syntax as output (@HsSyn Name@), albeit argmented
with additional information about identifiers.  After renaming, every
identifier in the abstract syntax is either an \emph{external} name,
qualified by the module that originally defined the name, or an
\emph{internal} name, defined by the current module.  More about the
datatypes that represent names in Section~\ref{s:names}.

Resolving names is the main job of the renamer, but it performs a
plethora of other tasks too: collecting the equations of a function
together and flagging an error if they have differing numbers of
arguments; rearranging infix expressions according to the fixity of
the operators; spotting duplicate declarations; generating warnings
for unused identifiers, and so on.

\subsubsection{Typechecking}

Type checking, as one might imagine, is the process of checking that
the Haskell program is type-correct.  If the program passes the type
checker, then it is guaranteed to not crash at runtime\footnote{unless
  it uses unsafe language features, such as the Foreign Function
  Interface}.

The input to the type-checker is @HsSyn Name@ (Haskell source with
qualified names), and the output is @HsSyn Id@.  An @Id@ is a @Name@
with extra information: notable a \emph{type}.  In fact, the Haskell
syntax produced by the type checker is fully decorated with type
information: every identifier has its type attached, and there is
enough information to reconstruct the type of any subexpression
(which might be useful for an IDE, for example).

In practice, type checking and renaming may be interleaved, because
the Template Haskell feature generates code at runtime that itself
needs to be renamed and typechecked.

\subsubsection{Desugaring, and the Core language}

Haskell is a rather large language, containing many different
syntactic forms.  It is intended to be easy for humans to read and
write - there is a wide range of syntactic constructs which gives the
programmer plenty of flexibility in choosing the most appropriate
construct for the situation at hand.  However, this flexibility means
that there are often several ways to achieve the same result: an @if@
expression is identical in meaning to a @case@ expression with @True@
and @False@ branches, and list-comprehension notation can be
translated into calls to @map@, @filter@, and @concat@.  In fact, the
definition of the Haskell language defines all these constructs by
their translation into simpler constructs; the constructs that can be
translated away like this are called ``syntactic sugar''.

It is much simpler for the compiler if all the syntactic sugar is
removed, because the subsequent optimisation passes that need to work
with the Haskell program have a smaller language to deal with.  The
process of desugaring therefore removes all the syntactic sugar,
translating the full Haskell syntax into a much smaller language that
we call @Core@.  The type of Haskell expressions, @HsExpr@, contains
40 syntactic forms, but @CoreExpr@ has only ten (and three of those
are purely annotations, dealing with types and profiling).

One interesting design decision is whether type-checking should be
done before or after Desugaring.  Certainly it is possible to do it
after desugaring, because the removal of syntactic sugar has no effect
on the type-correctness of the program, and furthermore it is simpler
to typecheck the @Core@ program, for the same reasons that it is
simpler to optimise @Core@.  But there is one major drawback to
typechecking @Core@: any type error messages displayed to the user
would be in terms of the @Core@ program, which may bear little
resemblance to the original source program.  For this reason, GHC's
type checker works with @HsSyn@ rather than @Core@, despite the extra
complexity that entails.

\subsubsection{Optimisation}

Now the program is in @Core@, the real business of optimisation
begins.  One of GHC's great strengths is in optimising away layers of
abstraction, and all of this work happens at the @Core@ level.  @Core@
is a tiny functional language, but it is a tremendously flexible
medium for expressing optimisations, ranging from the very high level,
such as strictness analysis, to the very low-level, such as strength
reduction.

Each of the optimisation passes takes @Core@ and produces
@Core@.  The main pass here is called the \emph{Simplifier}, whose job
it is to perform a large collection of correctness-preserving
transformations on the program, with the goal of producing a more
efficient program.  Some of these transformations are simple and
obvious, such as eliminating dead code or reducing a case expression
when the value being scrutinised is known, and some are more involved,
such as function inlining and applying rewrite rules.

The simplifier is normally run between the other optimisation passes,
of which there are about six; which passes are actually run and in
which order depends on the optimisation level selected by the user.

\subsubsection{Code Generation}

Once the @Core@ program has been optimised, the process of code
generation begins.  After a couple of administrative passes, the code
takes one of two routes: either it is turned into \emph{byte code} for
execution by the interactive interpreter, or it is passed to the
\emph{code generator} for eventual translation to machine code.

The code generator first converts the @Core@ into a language called
@STG@, which is essentially just @Core@ annotated with more
information required by the code generator.  Then, @STG@ is translated
to @Cmm@, a low-level imperative language with an explicit stack. From
here, the code takes one of three routes:

\begin{itemize}
\item \textbf{Native code generation}: GHC contains simple native code
  generators for a few processor architectures.  This route is fast,
  and generates reasonable code in most cases.

\item \textbf{LLVM code generation}: The @Cmm@ is converted to LLVM
  code and passed to the LLVM compiler.  This route can produce
  significantly better code in some cases, although it takes longer
  than the native code generator.

\item \textbf{C code generation}: GHC can produce ordinary C code.
  This route produces signficantly slower code than the other two
  routes, but can be useful for porting GHC to new platforms.
\end{itemize}

\section{Data structures}
\label{s:data}

\subsection{FastString}

%   - standard atom-like string storage, with one twist:
%   - UTF-8 encoded, cached mapping to Z-encoding

\subsection{Names, names, and names}
\label{s:names}

%   Tell the story about names: OccName, RdrName, Name, Id

\subsection{HsSyn: abstract syntax for Haskell}

% Abstract syntax parameterised over names
%   So that typecheck/rename can use the same data type for input and output

\subsection{Entities: TyCon, DataCon, Class, Id}

%  - why does everything point to everything else, rather than using
%    symbol tables?
%    + less information to carry around
%    + faster and more convenient to get to information quickly
%    - sometimes have to update the information inside entities
%      (e.g. typecheckLoop, and re-typechecking the interface even
%      if the module does not need to be recompiled)
%    - Have to be careful about sharing: if we substitute the type in an Id,
%      need to replace the occurrences too.
%    + we can make circular references easily in Haskell; this would not
%      be possible without circular references.

\section{GHC as a library: the GHC API}
\label{s:ghcapi}

One of GHC's original goals was to be a \emph{modular} foundation that
others could build on.  We wanted the code of GHC to be as transparent
and well-documented as possible, so that it could be used as the basis
for research projects by others; we imagined that people would want to
make their own modifications to GHC to add new experimental features
or optimisations.  Indeed, there have been many examples of this: for
example, there exists a version of GHC with a Lisp front-end, and a
version of GHC that generates Java code, both developed entirely
separately by individuals with little or no contact with the GHC team.

However, producing modified versions of GHC represents only a small
subset of the ways in which the code of GHC can be re-used.  As the
popularity of the Haskell language has grown, there has been an
increasing need for tools and infrastructure that understand Haskell
source code, and GHC of course contains a lot of the functionality
necessary for building these tools: a Haskell parser, abstract syntax,
type checker and so on.

With this in mind, we made a simple change to GHC: rather than
building GHC as a monolithic program, we build GHC as a
\emph{library}, that is then linked with a small \emph{Main} module to
make the GHC binary itself, but also shipped in library form so that
users can invoke its API from their own programs.  At the same time we
attempted to package GHC's funcionality into an easy-to-use API.  The
API provides enough functionality to implement the GHC batch compiler
and the GHCi interactive environment, but it also provides access to
individual passes such as the parser and type checker, and allows the
data structures produced by these passes to be inspected.  This change
has given rise to a wide range of tools built using the GHC API,
including:

\begin{itemize}
\item A documentation tool, \emph{Haddock}, which reads Haskell source
  code and produces HTML documentation
\item New versions of the GHCi front end with additional features,
\item IDEs that offer advanced navigation of Haskell source code,
\item \emph{hint}, a simpler API for on-the-fly evaluation of Haskell
  source code.
\end{itemize}

\section{The Runtime System}
\label{s:rts}

The Runtime System (hereafter, the RTS) is a library of mostly C code
that is linked into every Haskell program.  It provides the support
infrastructure needed for running the compiled Haskell code, including
the following main components:

\begin{itemize}
\item Memory management, including a parallel, generational, garbage collector;
\item Thread management and scheduling;
\item The primitive operations provided by GHC;
\item A bytecode interpreter and dynamic linker for GHCi.
\end{itemize}

The rest of this section is divided into two: first we focus on a
couple of the aspects of the design of the RTS that we consider to
have been successful and instrumental in making it work so well, and
secondly we talk about the coding practices and infrastructure we have
built in the RTS for coping with what is a rather hostile programming
environment.

\subsection{Key design decisions}

There are several design decisions in the RTS that we consider to have
been particularly successful, in the following sections we outline
two.

\subsubsection{The block layer}

The garbage collector is built on top of a \emph{block layer} that
manages memory in units of blocks, where a block is a multiple of 4KB
in size.  The block layer has a very simple API:

\begin{verbatim}
typedef struct bdescr_ {
    void *               start;
    struct bdescr_ *     link;
    struct generation_ * gen;   // generation
    // .. various other fields
} bdescr;

bdescr * allocGroup (int n);
void     freeGroup  (bdescr *p);
bdescr * Bdescr     (void *p);  // a macro
\end{verbatim}

This is the only API used by the garbage collector for allocating and
deallocating memory.  Blocks of memory are allocated with @allocGroup@
and freed with @freeGroup@.  Every block has a small structure
associated with it called a \emph{block descriptor} (@bdescr@).  The
operation @Bdescr(p)@ returns the block descriptor associated with an
arbitrary address @p@; this is purely an address calculation based on
the value of @p@ and compiles to a handful of arithmetic and
bit-manipulation instructions.

Blocks may be linked together into chains using the @link@ field of
the @bdescr@, and this is the real power of the technique.  The
garbage collector needs to manage several distinct areas of memory
such as \emph{generations}, and each of these areas may need to grow
or shrink over time.  By representing memory areas as linked lists of
blocks, the GC is freed from the difficulties of fitting multiple
resizable memory areas into a flat address space.

The implementation of the block layer uses techniques that are
well-known from C's @malloc()/free()@ API: it maintains lists of free
blocks of various sizes, and coalesces free areas.  The operations
@freeGroup()@ and @allocGroup()@ are O(1).

One major advantage of this design is that it needs very little
support from the OS, and hence is great for portability.  The block
layer needs to allocate memory in units of 1MB, aligned to a 1MB
boundary.  None of the common OSs provide this functionality directly,
but it is implementable without much difficulty in terms of the
facilities the OS provides.  The payoff is that GHC has no dependence
on the particular details of the addresss-space layout used by the OS,
and it coexists peacefully with other users of the address space, such
as shared libraries and operating system threads.

There is a small up-front complexity cost for the block layer, in
terms of managing chains of blocks rather than contiguous memory.
However, we have found that this is a cost is more than repaid in
flexibility and portabilty.

\subsubsection{Lightweight threads and parallelism}

We consider concurrency to be a vitally important programming
abstraction, particularly for building applications that interact with
large numbers of external agents simultaneously, such as web servers.
If concurrency is an important abstraction, then it should not be so
expensive that programmers are forced to avoid it, or build elaborate
infrastructure to amortise its cost (e.g. thread pools).  We believe
that concurrency should just work, and be cheap enough that you don't
worry about forking threads for small tasks.

All operating systems provide threads that work perfectly well, the
problem is that they are far too expensive.  Typical OSs struggle to
handle thousands of threads, whereas we want to manage threads by the
million.

Green threads, otherwise known as ligthweight threads or user-space
threads, are a well-known technique for avoiding the overhead of
operating system threads.  The idea is that threads are managed by the
program itself, or a library (in our case, the RTS), rather than by
the operating system.  Managing threads in user space should be
cheaper, because fewer traps into the operating system are required.

In the GHC RTS we take full advantage of this idea.  A context switch
only occurs when the thread is at a \emph{safe point}, where very
little additional state needs to be saved.  Because we use accurate
GC, the stack of the thread can be moved and expanded or shrunk on
demand.  Contrast these with OS threads, where every context switch
must save the entire processor state, and where stacks are immovable
so a large chunk of address space has to be reserved up front for
each thread.

Green threads can be vastly more efficient than OS threads, so why
would anyone want to use OS threads?  It comes down to three main
problems:

\begin{itemize}
\item Blocking and foreign calls.  A thread should be able to make a
  call to an OS API or a foreign library that blocks, without blocking
  all the other threads in the system.

\item Parallelism.  Threads should automatically run in parallel if
  there are multiple processor cores on the system.

\item Some external libraries (notably OpenGL and some GUI libraries)
  have APIs that must be called from the same OS thread each time,
  because they use thread-local state.
\end{itemize}

It turns out that all of these are difficult to arrange with green
threads.  Nevertheless, we persevered with green threads in GHC and
found solutions to all three:

\begin{itemize}
\item When a Haskell thread makes a foreign call, another OS thread
  takes over the execution of the remaining Haskell threads.  A small
  pool of OS threads are maintained for this purpose, and new ones are
  created on demand.

\item GHC's scheduler multiplexes many lightweight Haskell threads
  onto a few heavyweight OS threads; it implements a transparent M:N
  threading model.  Typically N is chosen to be the same as the number
  of processor cores in the machine, allowing real parallelism to take
  place but without the overhead of having a full OS thread for each
  lightweight Haskell thread.

  In order to run Haskell code, an OS thread must hold a
  \emph{Capability}\footnote{we have also called it a ``Haskell
    Execution Context'', but the code currently uses the Capability
    terminology}: a data structure that holds the resources required
  to execute Haskell code, such as the nursery (memory where new
  objects are created).  Only one OS thread may hold a given
  Capability at a time.

\item We provide an API for creating a \emph{bound thread}: a Haskell
  thread that is tied to one specific OS thread, such that any foreign
  calls made by this Haskell thread are guaranteed to be made by that
  OS thread.
\end{itemize}

So in the vast majority of cases, Haskell's threads behave exactly
like OS threads: they can make blocking OS calls without affecting
other threads, and they run in parallel on a multicore machine.  But
they are orders of magnitude more efficient, in terms of both time and
space.

Having said that, the implementation does have one problem that users
occasionally run into, especially when running benchmarks.  We
mentioned above that lightweight threads derive some of their
efficiency by only context-switching at ``safe points'', points in the
code that the compiler designates as safe, where the internal state of
the virtual machine (stack, heap, registers etc.) is in a tidy state
and garbage collection could take place.  In GHC, a safe point is
whenever memory is allocated; which in almost all Haskell programs
happens regularly enough that the program never executes more than a
few tens of instructions without hitting a safe point.  However, it is
possible in highly optimised code to find loops that run for many
iterations without allocating memory.  This tends to happen often in
benchmarks (e.g. functions like factorial and fibonacci).  It occurs
less often in real code, although it does happen.  The lack of safe
points prevents the scheduler from running, which can have detrimental
effects.  It is possible to solve this problem, but not without
impacting the performance of these loops, and often people care about
saving every cycle in their inner loops.  This may just be a
compromise we have to live with.


\subsection{Developing the RTS}

GHC's runtime system presents a stark contrast to
the compiler in many ways.  There is the obvious difference that the
runtime system is written in C rather than Haskell, but there are also
considerations unique to the RTS that give rise to a different design
philosophy:

\begin{enumerate}
\item Every Haskell program spends a lot of time executing code in the
  RTS: 20--30\% is typical, but characteristics of Haskell
  programs vary a lot and so figures greater or less than this range
  are also common.  Every cycle saved by optimising the RTS is
  multiplied many times over; so it is worth spending a lot of time
  and effort to save those cycles.

\item The runtime system is statically linked into every Haskell
  program\footnote{that is, unless dynamic linking is being used}, so
  there is an incentive to keep it small.

\item Bugs in the runtime system are often inscrutable to the user
  (e.g. ``segmentation fault'') and are hard to work around.  For
  example, bugs in the garbage collector tend not to be tied to the
  use of a particular language feature, but arise when some complex
  combination of factors emerges at runtime.  Furthermore, bugs of
  this kind tend to be non-deterministic (only occurring in some
  runs), and highly sensitive (tiny changes to the program make the
  bug disappear).  Bugs in the multithreaded version of the runtime
  system present even greater challenges.  It is therefore worth going
  to extra lengths to prevent these bugs, and also to build
  infrastructure to make identifying them easier.

  The symptoms of an RTS bug are often indistinguishable from two
  other kinds of failure: hardware failure, which is more common than
  you might think, and misuse of unsafe Haskell features like the FFI.
  The first job in diagnosing a runtime crash is to rule out these two
  other causes.

\item The RTS is low-level code that runs on several different
  architectures and operating systems, and is regularly ported to
  new ones.  Portability is important.
\end{enumerate}

Every cycle and every byte is important, but correctness is even more
so.  Moreover, the tasks performed by the runtime system are
ineherently complex, so correctness is hard to begin with.
Reconciling these has lead us to some interesting defensive
techniques, which we describe in the following sections.

\subsubsection{Coping with complexity}
\label{s:rtsbugs}

The RTS is a complex and hostile programming environment.  In contrast
to the compiler, the RTS has almost no type safety.  In fact, it has
even less type safety than most other C programs, because it is
managing data structures whose types live at the Haskell level and not
at the C level.  For example, the RTS has no idea that the object
pointed to by the tail of a cons cell is either @[]@ or another cons:
this information is simply not present at the C level.  Moreover, the
process of compiling Haskell code erases types, so even if we told the
RTS that the tail of a cons cell is a list, it would still have no
information about the pointer in the head of the cons cell.  So the
RTS code has to do a lot of casting of C pointer types, and it gets
very little help in terms of type safety from the C compiler.

So our first weapon in this battle is to \emph{avoid putting code in
  the RTS}.  Wherever possible, we put the minimum amount of
functionality into the RTS and write the rest in a Haskell library.
This has rarely turned out badly: Haskell code is far more robust and
concise than C, and performance is usually perfectly acceptable.

That still leaves plenty of functionality that can't be implemented in
Haskell, though, and writing code in the RTS is not pleasant.  In the
next sections we focus on a couple of aspects of managing complexity
and correctness in the RTS: maintaining invariants, and debugging
multithreaded code.

\subsubsection{Invariants, and checking them}

The RTS is full of invariants.  Many of them are trivial and easy to
check: for example, if the pointer to the head of a queue is @NULL@,
then the pointer to the tail should also be @NULL@.  The code of the
RTS is littered with assertions to check these kinds of things.
Assertions are our go-to tool for finding bugs before they manifest;
in fact, when a new invariant is added, we often add the assertion
before writing the code that implements the invariant.

Some of the invariants in the runtime are far more difficult to
satisfy, and to check.  One invariant of this kind that pervades more
of the RTS than any other is the following: \emph{the heap has no
  dangling pointers}.

Dangling pointers are easy to introduce, and there are many places
both in the compiler and the RTS itself that can violate this
invariant.  The code generator could generate code that creates invalid
heap objects; the garbage collector might forget to update the pointers
of some object when it scans the heap.  Tracking down these kinds of
bugs can be extremely time consuming\footnote{it is, however, one of
  the author's  favourite activities!} because by the time the program eventually
crashes, execution might have progressed a long way from where the
dangling pointer was originally introduced.  There are good debugging
tools available, but they tend not to be good at executing the program
in reverse.\footnote{recent versions of GDB do have some support for
  reverse execution, however.}

The general principle is: \emph{if a program is going to crash, it
  should crash as soon, as noisily and as often as
  possible.}\footnote{This quote comes from the GHC coding style
  guidelines, and was originally written by Alastair Reid, who worked
  on an early version of the RTS.}

The problem is, the no-dangling-pointer invariant is not something
that can be checked with a constant-time assertion.  The assertion
that checks it must do a full traversal of the heap!  Clearly we
cannot run this assertion after every heap allocation, or every time
the GC scans an object (indeed, this would not even be enough, as
dangling pointers don't appear until the end of GC, when memory is
freed).

So, the debug RTS has an optional mode that we call \emph{sanity
  checking}.  Sanity checking enables all kinds of expensive
assertions, and can make the program run many times more slowly.  In
particular, sanity checking runs a full scan of the heap to check for
dangling pointers (amongst other things), before \emph{and} after
every GC.  The first job when investigating a runtime crash is to run
the program with sanity checking turned on; sometimes this will catch
the invariant violation well before the program actually crashes.

\subsubsection{Debugging the multithreaded RTS}

GHC supports parallel and concurrent execution of Haskell programs,
and contains its own thread scheduler.  In fact, in many ways the GHC
RTS is a small operating system.

GHC comes with both single-threaded and multithreaded variants of the
RTS.  Concurrent Haskell programs run perfectly well with the
single-threaded RTS: the RTS scheduler runs the Haskell threads in
round-robin fashion on its one and only OS thread.  The only downside
to doing this is that when one Haskell thread is making a foreign
call, no other Haskell threads can run until the foreign call returns.
The mulitthreaded RTS relaxes this restriction: when a Haskell thread
makes a foreign call, another OS thread takes over the execution of
the other Haskell threads in the system.  In fact, there can be
multiple OS threads running Haskell threads at the same time, which is
the only way for concurrent or parallel programs to achieve a speedup
on a multi-core machine.

Diagnosing and fixing failures in the multithreaded RTS presents
some significant challenges.  If we're lucky, inspecting the state of
the RTS internals in the debugger at the time of the crash is enough
to diagnose the bug, but if not, things are much more difficult:

\begin{itemize}
\item First, it is often impossible to provoke the failure
  deterministically.  To provoke the failure we may have to run the
  program many times.

\item Therefore, our usual tricks for going back in time, such as
  stopping at the GC just before the failure, don't work.

\item Sanity checking can be useful, but sometimes this distorts the
  execution profile of the program sufficiently that the bug stops
  manifesting.
\end{itemize}

Perhaps our most potent tool is assertions: if the crash indicates
that some invariant has been violated, we add assertions to check the
invariant at more places in the code, and hope to narrow down the
cause of the failure that way.

Sometimes that isn't enough, and we need more insight into what the
program was doing shortly before it crashed.  The RTS of course can
produce copious amounts of debugging output, organised into different
classes and selectable by flags at runtime.  However, this turns out
to be less than useful for finding bugs in multithreaded programs: the
act of generating the debugging output slows down the program and adds
synchronisation, which often stops the bug from manifesting.

This is where GHC's \emph{event logging} infrastructure comes in
handy: the RTS can log events in a binary format, using per-thread
buffers to avoid synchronisation.  This feature was added primarily
for profiling---the outpout is used by our parallel profiler
\emph{ThreadScope}---but it turns out to be invaluable for debugging,
too.

\section{Retrospective}

\subsection{C is not a good target language}

% - Getting good performance while targetting C is too hard
%   - tail calls
%   - registers
%   - shadow stack
%
% - Compiling via C is slow
%
% - Portability: C is good here, but LLVM is better now
%
% - Compiling to LLVM is better, but still not quite a perfect fit:
%   LLVM wants to manage the stack, we have to avoid that.
%
% - Native back end compiles quickly, but generates mediocre code
%   (perfectly acceptable for most programs)

\subsection{How to keep on refactoring}

The code of GHC is churning just as quickly, if not more so, than it
was ten years ago.  There is no doubt that the complexity of the
system has increased manifold over that same time period: we saw
measures of the amount of code in GHC earlier.  Yet, the system
remains manageable.  We attribute this to three main factors:

\begin{itemize}
\item There's no substitute for good software engineering.  Modularity
  always pays off: making the APIs between components as small as
  possible makes the individual components more flexible because they
  have fewer interdependencies.  For example, GHC's Core datatype
  being small reduces the coupling between Core-to-Core passes, to the
  extent that they are almost completely independent and can be run in
  an arbitrary order.

\item \emph{Strong typing} makes refactoring a breeze.  Whenever we
  need to change a data type, or change the number of arguments or
  type of a function, the compiler immediately tells us what other
  places in the code need to be fixed.  Simply having an absolute
  guarantee that a large class of errors have been statically ruled
  out gives a tremendous feeling of security, especially when
  refactoring.

\item When programming in a \emph{pure functional} language, it is
  hard to introduce accidental dependencies via state.  If you decide
  that you suddenly need access to a piece of state deep in an
  algorithm, in an imperative language you might be tempted to just
  make the state globally visible rather than explicitly pass it down
  to the place that needs it.  This way eventually leads to a tangle
  of invisible dependencies, and \emph{brittle code}: code that breaks
  easily when modified.  Pure functional programming forces you to
  make all the dependencies explicit, which exerts some negative
  pressure on adding new dependencies, and fewer dependencies means
  greater modularity.  Certainly when it is \emph{necessary} to add a
  new dependency then purity makes you write more code to express the
  dependency, but in our view it is a worthwhile price to pay for the
  long-term health of the code base.

  As an added benefit, purely functional code is thread-safe by
  construction and parallelises for free.
\end{itemize}

% *** How did writing in a (pure) functional language affect the design?
%    - lets us make circular references easily (see data structures)
%
%    - we use mutation heavily in the type checker, but other passes
%      are pure.
%
%    - splittable name supply trick

\subsection{Crime doesn't pay}

Looking back over the changes we've had to make to GHC as it has
grown, a common lesson emerges: being less than purely functional,
whether for the purposes of efficiency or convenience, tends to have
negative consequences down the road.  We have a couple of great
examples of this:

\begin{itemize}
\item GHC uses a few data structures that rely on mutation internally.
  One is the @FastString@ type, which uses a single global hash table
  (Section~\ref{s:data}); another is a global @NameCache@ that ensures
  all external names are assigned a unique number.  When we tried to
  parallelise GHC (that is, make GHC compile multiple modules in
  parallel on a multicore processor), these data structures based on
  mutation were the \emph{only} sticking points.  Had we not resorted
  to mutation in these places, GHC would have been almost trivial to
  parallelise.

  In fact, although we did build a prototype parallel version of GHC,
  GHC does not currently contain support for parallel compilation, but
  that is largely because we have not yet invested the effort required
  to make these mutable data structures thread-safe.

\item GHC's behaviour is governed to a large extent by command-line
  flags.  These command-line flags are by definition constant over a
  given run of GHC, so in early versions of GHC we made the values of
  these flags available as top-level constants. For example, there was
  a top-level value @opt_GlasgowExts@ of type @Bool@, that governed
  whether certain language extensions should be enabled or not.
  Top-level constants are highly convenient, because their values
  don't have to be explicitly passed as arguments to all the code that
  needs access to them.

  Of course these options are not really \emph{constants}, because
  they change from run to run, and the definition of @opt_GlasgowExts@
  involves calling @unsafePerformIO@ because it hides a side effect.
  Nevertheless, this trick is normally considered ``safe enough''
  because the value is constant within any given run; it doesn't
  invalidate compiler optimisations, for example.

  However, GHC was later extended from a single-module compiler to a
  multi-module compiler.  At this point the trick of using top-level
  constants for flags broke, because the flags may have different
  values when compiling different modules.  So we had to refactor
  large amounts of code to pass around the flags explicitly.

  Perhaps you might argue that treating the flags as \emph{state} in
  the first place, as would be natural in an imperative language,
  would have sidestepped the problem.  This is true, but as we argued
  earlier, purely functional code has a number of other benefits;
  benefits that we are not prepared to abandon solely because we want
  read-only access to some flags.
\end{itemize}

\subsection{Lazy evaluation}

% *** How did _laziness_ affect the design
%
%    - The native code generator is lazy: code is generated as it is
%      written to the output file, not all at once (saves a lot of
%      space, native code is much bigger than Cmm)
%        - BUT, this property is easy to break, we have done so several
%          times, and we now have a regression test for it.
%
%    - laziness is used to read in and typecheck interface files on demand
%      (is this a *good* design??)
%
%    - finding space leaks is tricky: we have lots of strict fields and
%      explicit seqing of data structures between passes (and these
%      data structures are *deep*, see description of data structures).
%

\subsection{What would we do differently?}

%    - Generic traversals (maybe).



\section{Libraries}

% We have to include in the source distribution, and ship, libraries
% that GHC depends on.
%
%  - we need to be able to bootstrap GHC from a single source tree
%  - we want to offload development effort for key libraries,
%    but benefit from them in GHC itself (containers, bytestring)
%
% But this means that
%
%  - making local changes is difficult
%  - we have to comply with the licenses
%  - it's a big deal to add a dependency on a new library
%  - upgrading these libraries later is hard, because the GHC package
%    depends on the old one.

\section{Bootstrapping}

% Interesting things to say about bootstrapping?
%  - sometimes a released GHC has a bug, which only shows up later and
%    we have to work around it (e.g. the 7.2.1 case-floating bug)

\section{Tools and development practices}

% Greg said they only want this kind of stuff if it is different and
% interesting.
%
% * build systems?  How does bootstrapping make things interesting?
% (2 or 3-stage build, now built into the build system, originally
% needed separate trees).
%
%    * non-recursive make, previously: Imake, plain GNU make.
%
% * Validate?

\end{document}
