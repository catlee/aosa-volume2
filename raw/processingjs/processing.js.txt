[THE ARCHITECTURE OF OPEN SOURCE APPLICATIONS]

THE PROCESSING AND PROCESSING.JS PROJECTS

  Originally developed by Ben Fry and Casey Reas, the Processing programming language started as an open source programming language based on Java to help the electronic arts and visual design communities learn the basics of computer programming in a visual context. Offering a highly simplified model for 2D and 3D graphics compared to most programming language, it quickly became well suited for a wide range of activities, from teaching programming through writing small visualisations to creating multi-wall art installations, and performing a wide variety of tasks, from simply reading in a sequence of strings to acting as the de facto IDE for programming and operating the popular "Arduino" PCBs. All the while gaining in popularity, Processing has firmly taken its place as an easy to learn, widely used programming language for all things visual, and so much more. 

  Processing.js is the sister project of the popular Processing visual programming language, designed bring Processing to the web without a need for java or plugins. It started as an attempt by John Resig to see if the Processing language could be ported to the web, using the HTML5 "canvas" element as a graphical context, with a proof of concept library released to the public in 2008. Written with the idea in mind that "your code should just work", Processing.js has been refined over the years to make your data visualizations, digital art, interactive animations, educational graphs, video games, etc. work using web standards and without any plug-ins. You write code using the Processing language, either in the Processing IDE or your favourite editor of choice, include it on a web page using a <canvas> element, and Processing.js does the rest, rendering everything in the canvas element and letting users interact with the graphics in the same way they would with a normal stand-alone Processing sketch.

BUILDING PROCESSING.JS

  To WRITE SOFTWARE, YOU NEED PEOPLE

    Processing.js is a classic example of open source software in the way it sees contributors come and go. As the project is about providing a library for parsing and running an established programming language in a web setting, there is no hard distinction between "users" and "developers". If you write a Processing "sketch" and discover it doesn't work when you run it through Processing.js, the line between reporting a bug with guesses on what would fix it, and actually fixing a bug, is a fine one. User interaction is mainly taken care of in the form of the #processing.js IRC channel on mozilla's irc.mozilla.org IRC server, and anyone who takes the time to find us on IRC is usually willing to not just report the problem they are having, but also help solve it. While the difference in programming languages --JavaScript for Processing.js versus a Java API for Processing-- may prevent some from diving into the library with us to figure out the problem, the fact that virtually all users that drop in to report problems are able to program means that bug discussions take place on a high technical level, blurring the line between "user", "beta tester", and "developer" from bug to bug.

    In the spirit of open source, people who report problems often stick around for a while, to help further the project in small ways, usually by trying to get their Processing projects working using processing.js and reporting their findings when Processing.js fails. These short term contributions help build out the project by showing us how people use Processing in the wild, showing us the various ways in which people write Processing programs, and letting us refine our parser code to deal with the various interprations of "code syntax" as well as deal with complex interaction behaviours that only show up in larger programs, revealing inconsistencies or deficiencies in how we implemented the porting of Processing and Java code to JavaScript.
    
    Sometimes we're lucky, and someone drops in to report a problem as well as offer a potential patch or conceptual solution (this is, incidentally, how I joined the projject myself). Even though the JavaScript language that Processing.js is written in is fairly different in its working from Java, knowing one lets you tinker in the other, which allows people to pull up the library's source code and, if sufficiently motivated, tinker with it until their problem is solved.

    However, sometimes a problem is big. Really big. And these are the times when Processing.js tends to gain a developer, simply because the result will be worth it. Proper parsing of Object Oriented code, inheritance and hierarchies, making 3D graphics work via WebGL, allowing proper font handling, enabling SVG and XML compatibility, all these things "just work" in native Processing, but have required substantial work in the Processing.js project simply because JavaScript or the browser handles things either subtly different, or inherently not at all. People who have relied on these things in their own work and wanted to make the jump from Java applets to pure web technologies have come and gone to help us implement these things, and get them right.

    While it's great to have people jumping on board to help fix bugs and implement missing features, the greatest benefit is that virtually everyone who works with Processing works with more than one underlying technology to achieve their goals. This means that someone reporting a problem relating to inheritance might also be used to working with XML, and someone who's finding bugs in the WebGL 3D implementations may also know things about the math required to efficiently perform 2D operations. This means that rather than having everyone work on their own little island, bugs tend to be tackled through concerted efforts by several people, with one person writing what they think is a good fix, and another person offering a potentially completely different solution to the same problem. Not only does this keep everyone involved, but it also lets people learn from the work of others. Rather than simply slamming out code, everyone working on Processing.js is offered ample opportunity to learn all the crazy things that JavaScript can do, as well as discsover many new programming patterns and ways of thinking about problems. Our mode of operation heavily relies on contributors with a diverse background and interest, who aren't afraid to get something wrong if it means learning how to do things right.
    
    Serving a dual purpose as both a real world project as well as an educational project under David's open source courses, everyone learns to explain their work as they go along, which has lead to many of the Processing.js developers to also be excellent instructors, even if only for the code they themselves have written. Rather than simply write code and be done with it, the way development works in Processing.js almost naturally leads to developers being able to explain not just way their code works, but also why it was written the way it was, and what the tradeoffs are between different implementations.


  THE DEVELOPMENT PROCESS

    Unlike some open source projeccts, Processing.js has a remarkably clear goal, with a constantly visible finishing line: parity with native Processing. There are several stages to an open source project, and Processing.js has gone through most of these: 
    
    - an initial attempt at a proof of concept,
    - a refinement of the initial attempt to suit user needs,
    - incremental work to fill in the major blanks in order to reach that fabled 1.0 release,
    - an immediate realisation that 1.0 wasn't actually done at all,
    - a complete rewrite of most parts to take advantage of new developers and advances in technologies,
    - a gradual move from development to maintenance, and finally,
    - an audit of the entire project to determine how to achieve perfection.
    
    Different stages come with difference development models. Early in the development stage the focus is on implementing as many of the core Processing functions as possible, just to get things functionally working, even if it's slow. As time goes on, this mad rush for functionality makes way for code improvement, both as optimisation and adding support for things that deserve their own minor release --things like full object oriented support, clean separation of 2D and 3D contexts, XML and SVG support-- until at some point even all the major complex issues have been dealt with, and the focus moves to underwater improvements, making the code perform better while reducing its footprint.

    Of course, with native Processing being an open source project itself, the project that Processing.js seeks parity with has changed over time, too. The API has changed several times, and the way things work in Processing has changed more than once over time, forcing Processing.js to change accordingly. Every now and then, the fact that Processing changes in API-changing ways means that we have to take a few steps back for parts of the project, and sometimes abandon what we had in favour of new and untested code, spending a great deal of time going through the first few steps of the open source development track. Consequently, some parts of the Processing.js code base will fall in the maintenance and optimization category, while other parts are so new that they're a breeding ground for new bug reports and repeated attempts at getting things right.

    However, even though an open source project goes through several stages from initial mostly-new-development to being mostly maintenance, the guiding principles behind development of Processing.js have remained the same:

      "if there is a problem, file it."

    Even small issues that just pop up as asides in the IRC channel during a discussion on how to tackle certain problems are met with the two most important words for any development process: "File it." While IRC is an excellent development environment because of its real time interaction (without a need to call meetings or arrange teleconferences!), IRC is also an asynchronous, stateless medium. Any time anyone makes a comment about Processing.js that might normally be lost because it's just an off-topic remark, they will be told to file a ticket. Sometimes it's just something small, such as a missing bit of documentation, or a typo in the source code's comments, but sometimes it's the kind of remark that can spark months of on-and-off development by several contributors. These remarks are usually easy to spot because they tend to take the form of "I wonder how hard it would be to implement ..." and "You know what would be cool? If we could ..."

    combining IRC for the development dialogue and ticket filing for our explicit todo list, means that nothing is lost. In addition to the usual idea where tickets are filed, fixed, and marked as resolved, we take our todo list even more serious: Any ticket that involves development is an opportunity for us to better our development environment, to ensure Processing.js works. This is based on three principles:

      1) make it work,
      2) make it fast, and
      3) make it small.

    These sound like obvious principles, but they come with some strings attached.


  MAKE IT WORK

    Writing code that works sounds like a tautological premise; you write code, and by the time you're done your code either works, because that's what you set out to do, or it doesn't, and you're not done yet. However, "make it work" comes with a corrolary.

    "Make it work. And when you're done, prove it."

    If there is one thing above all other things that has allowed Processing.js to grow at the pace it has, it is the presence of tests. Any ticket that requires touching the code, be it either by writing new code, or rewriting old code, cannot be marked as resolved until there is a unit or reference test that allows others to verify not only that the code works the way it should, but also that it breaks when it should. For most code, this typically involves a unit test - a short bit of code that calls a function and simply tests whether the function returns the correct values, for both legal and illegal function calls. Not only does this allow us to test code contributions, but it also lets us perform regression tests. Before any code is accepted and merged into our stable development branch, the modified Processing.js library is validated against an ever-growing battery of unit tests. Especially big fixes and performance tests are prone to pass their own unit test, but may end up breaking parts that worked fine before the rewrite. Having tests for every function in the API, as well as internal functions, means that as Processing.js grows, we don't accidentally break compatibility with previous versions. Barring destructive API changes, if none of the tests failed before a code contribution or modification, none of the tests are allowed to fail with the new code in.

  ((Unit test example box - ))

    In addition to regular code unit tests, we also have visual reference tests. As Processing.js is a port of a visual programming language, some tests cannot be performed using just unit tests. Testing to see whether an ellipse gets drawn on the correct pixels, or whether a single-pixel-wide vertical line is drawn crisp or smoothed cannot be determined without a visual reference. Because all big browsers implement the canvas element and Canvas2D API subtly differently, these things can only be tested by running code in a browser and verifying that the resulting sketch looks the same as what native Processing generates. To make life easier for developers, we use an automated test suite for this, where new test cases are run through Processing, generating "what it should look like" data to be used for pixel comparison. This data is then stored as a comment inside the sketch that generated it, forming a test, and these tests are then run by Processing.js on a visual reference test page which executes each test and performs pixel comparisons between "what it should look like" and "what it looks like". If the pixels are off, the test fails, and the user is presented with three images; one showing what it should look like, one showing how Processing.js rendered it, and one showing the difference between the two, marking problem areas as red pixels, and correct areas as white. Much like unit tests, these tests must pass before any code contribution can be accepted.

  ((ref test illustration - ref test page screenshot))

  MAKE IT FAST

    In an open source project, making things work is only the first step in the life of a function. Once things work, you want to make sure things work fast. Based on the "if you can't measure it, you can't improve it" principle, most functions in Processing.js don't just come with unit or ref tests, but also with performance tests. Small bits of code that simply call a function, without testing the correctness of the function, are run several hundred times in a row, with the time required for them to run recorded on a special performance test web page. This lets us quantify how well (or not!) Processing.js performs in browsers that support HTML5's <canvas> element. Every time an optimization patch passes unit and ref testing, it is run through our performance test page. JavaScript is a curious beast, and beautiful code can, in fact, run several orders of magnitude slower than code that contains the same lines several times over, with inline code rather than function calls. This makes performance testing crucial. We have been able to speed up certain parts of the library by three orders of magnitude simply by discovering hot loops during perf testing, and reducing the number of function calls by inlining code, and by making functions return the moment they know what their return value should be, rather than having only a single return at the very end of the function.


  MAKE IT SMALL

    There are two ways to make code small. Foremost, write compact code. If you're manipulating a variable multiple times, compact it to a single manipulation (if possible). If you access an object variable multiple times, cache it. If you call a function multiple times, cache the result. Return once you have all the information you need, and generally apply all the tricks a code optimiser would apply yourself. Javascript is a particularly nice language for this, since it comes with an incredible amount of flexibility. For example, rather than using "if((result = functionresult)!==null) { var = result; } else { var = default;}" in JavaScript this becomes "var = functionresult || default".
    
    There is also another form of small code, and that's in terms of runtime code. Because JavaScript lets you change function bindings on the fly, running code becomes much smaller if you can say "bind the function for line2D to the function call for line()" once you know that a program runs in 2D rather than 3D mode, so that you don't have to perform "if(mode==2D) { line2D() } else { line3D() }" for every function call that migth be either in 2D or 3D mode.
    
    Finally, there is the process of minification. There are a number of good systems available that lets you compress your javascript code by renaming variables, stripping whitespace, and applying certain code optimisations that are hard to do by hand while still keeping the code readable. Examples of these are the YUI minifier and google's closure compiler. We use these technologies in Processing.js to offer end users bandwidth convenience - minification after stripping comments can shrink the library by as much as 50%, and taking advantage of modern browser/server interaction for gzipped content, we can offer the entire Processing.js library in gzipped form in 65KB.


  WIELDING THE DUAL SWORDS OF LIGHTHOUSE AND GITHUB

    Effective code development relies on efficient issue tracking and code maintenance, which we do through the Lighthouse issue tracker, and the git versioning system for code. On the lighthouse side of things, tickets are filed based on the simple philosophy that if it's an issue, it gets its own ticket. This means that when someone files a ticket for which more than one function needs to be modified, or for some new functionality requires both code as well as documentation, the ticket will be set "blocked" on two new tickets that deal with the individual issues. This allows us to easily roll back fixes when we have to (and as with all good open source projects, this happens more than we would like). On the filing side of things, people who want to report a bug are not asked for a plethora of details before they can tell us what's wrong: a title and description of the problem is enough, and the entire Processing.js team shares the task of looking at the newly filed tickets bin to determine which ticket goes with what milestone, and whether all the information we need to start working on a ticket is there. If it's not, we simply ask for the relevant information in the ticket. 
    
    For code contributions we rely on git and the power of github. There are effectively two major branches for Processing.js: the current release branch, hosted as a dedicated github project, and the development branch, which is considered a stable snapshot of the code in between releases, touched only when a patch that has passed testing is merged into it. Any ticket that requires working with the code branches off of the development branch, with separate branches for each ticket that gets worked on, and everyone who wants to contribute can fork the Processing.js project from whoever is considered the lead developer (or at least whose account is considered to house the development branch of Processing.js), creating individual branches for each ticket. Lighthouse stores tickets on ticket number as well as ticket number and title, and in general a ticket with number #1234 will be worked on by someone in a branch t1234. One of the interesting things you learn about git is that "git checkout <branch>" works great until the branch name can also be interpreted as a hexadecimal commit hashcode, and so there was a need for some leading non-hexadecimal string. The "t" won.
    
    After writing a bug fix or code contribution, a comment is filed on lighthouse with the links to the github tree and commit URLs, and the ticket enters a review cycle. Rather than test once, tickets in Processing.js are first set to a peer review request, which involves testing against the unit and visual ref tests, as well as looking at the code to make sure it conforms to the coding standard used in our project, as well as to see whether there are still improvements left to be made. If it passes peer review, a super review is requested. In this step the code is scrutinized to make sure everything checks out, and if the unit and ref tests all pass, only then is the ticket considered resolved, and marked as "looks good", so that it may be merged into the master development branch using git's merge commands.


  IF ALL ELSE FAILS, TELL PEOPLE

    Not everything that can currently be done in Processing can be done in the browser. Security models prevent certain things like saving files to the harddisk and performing USB or Serial port I/O, and a lack of typing in Javascript can have unexpected consequences (such as all math being floating point math). Sometimes we're faced with the choice between adding an incredible amount of code to enable an edge case, or mark the ticket as a "wontfix" issue. In such cases, a new ticket gets filed typically titled "add documentation that explains why ...".
    
    In order to make sure these things aren't lost, we have documentation online for people who start using Processing.js with a Processing background, and people who start using Processing.js with a JavaScript background, covering the differences between what is expected, and what actually happens. Certain things just deserve special mention, because no matter how much work we put into Processing.js, there are certain things we cannot add without sacrificing usability.


HOW DOES IT WORK?

  Processing.js is a bit unusual as Open Source project in that the codebase is actually not a codebase, but just a single file, "processing.js", with a big source code object description inside it, the "Processing" object. In terms of how the code is structured, we constantly shuffle things around inside this object as we try to clean it up a little bit with every release, but it's actually modelled relatively straight foward, and its function can be described in a single sentence: it rewrites Processing (Java) source code into pure JavaScript source code, and every Processing API function call is mapped to a corresponding function in the Processing object, which effects the same thing on a <canvas> element as the Processing call would effect on a Java Applet canvas.
  
  For speed, we have two seprate code subsets for 2D and 3D functions, and when a sketch is loaded either one or the other is used for resolving function wrappers so that we don't add bloat to running instances, but in terms of datastructures and code flow, if you know JavaScript, you can read Processing.js -- except, perhaps, for the parser.

  UNIFYING JAVA AND JAVASCRIPT

    Rewriting Processing source code into JavaScript source code means that you can simply tell the browser to execute the rewritting source, and if you rewrote it correctly, things just work. But making sure the rewrite is correct has taken, and still occasionally takes, quite a bit of effort: Processing syntax is based on Java syntax, which means that the library has to essentially transform Java code into JavaScript code. Initially, this was achieved through iterative replacements of the source code as text string (for those interesting in the early incarnation of the parser, you can find it at https://github.com/jeresig/processing-js/blob/51d280c516c0530cd9e63531076dfa147406e6b2/processing.js#L37 running up to line 266), slowly turning it from a Java string into a JavaScript string. For a small syntax set, this is fine, but as time went on and complexity added on complexity, this approach started to break down, and the parser was completely rewritten to build an Abstract Syntax Tree (AST) instead, first breaking down the Java source code into functional blocks, and then mapping each of those blocks to corresponding JavaScript syntax. The result is that, at the cost of readability (readers are welcome to peruse https://github.com/jeresig/processing-js/blob/v1.3.0/processing.js#L17649 up to line 19217) Processing.js now effectively contains a complete on-the-fly Java-to-JavaScript converter.
    
  [source blocks showing a small Processing sketch and its corresponding javascript source]

    This sounds like a great thing, but there's a few problems when converting Java syntax to JavaScript syntax:
    
    1) Java programs are isolated entities. JavaScript programs share the world with a web page.
    2) Java is strongly typed. JavaScript is not.
    3) Java is a class/instance based OO language. JavaScript is not.
    4) Java has distinct variables and methods. JavaScript does not.
    5) Java allows method overloading. JavaScript does not.
    6) Java allows importing compiled code. JavaScript has no idea what that even means.

    Dealing with these problems has been a tradeoff between what users need, and what we can do given web technologies.
    
    JAVA PROGRAMS ARE ISOLATED ENTITIES. JAVASCRIPT PROGRAMS SHARE THE WORLD WITH A WEB PAGE
    
    When a Java program loads a file, the program waits until the resource is done loading, and operation resumes as intended. In a setting where the program is an isolated entity on its own, this is fine. The Operating system stays responsive because it's responsible for thread scheduling, and even if the program takes an hour to load all its data, you can still use your computer. On a web page, this is not how things work. If you have a javascript "program" waiting for a resource to be done loading, it will lock up the entire browser process. If you're using a browser that uses one process per tab, it'll lock up your tab, and the rest of the browser is still usable, but regardless of what the process represents, the page the script runs on won't be usasble until the resource is done loading. This is unacceptable on the modern web, where resources are transfered asynchronously, and the page just "does things" while resources are loaded. While this is great for web pages, for applications this is a real brain twister: how do you "do nothing" while a resource loads asynchronously? JavaScript offers the XMLHTTPRequest object, which gets a notification once data is available, but the whole idea of this is that there is a function waiting to be called once the load is done. What do you do for dynamic load instructions from arbitrary functions?
    
    For some things, we decided to do synchronous waiting. Loading a file with strings, for instance, uses a synchronous ajax request, and will halt execution of the page until the data's available. For other things, we had to get creative. Loading images, for instance, uses the browser's build in mechanism for loading images - we build a new Image() in JavaScript, set its src attribute to the image url, and the browser does the rest. This doesn't even rely on an XMLHTTPRequest, but simply exploits the browser's capabilities. An onload event on the image variable then norifies us that the image is done transfering AND ready to be rendered (rather than simply having been downloaded but not decoded as pixel array in memory yet), after which we can populate the corresponding Processing "PImage" object with the correct values - width, height, pixel data, etc.
    
    For some other things, we've had to build in a "wait for me" system. For fonts, as well as for images that should be available already when the sketch starts, we added preloading directives that take the form of a block comment at the top of the sketch code. When these are encountered, loading of the sketch is deferred, and images and fonts are downloaded via browser instructions --new Image() with image.src assignment for images, and css @font-face inclusion for custom fonts-- executing the sketch only after these resources are known to have finished downloading, and have been loaded in memory for use.


    JAVA IS STRONGLY TYPED. JAVASCRIPT IS NOT.
    
    In Java, the number "2" and the number "2.0" are different values, and they will do different things during mathematical operations. "1/2" in Java is 0, because the numbers are treated as integers, whereas '1.0/2.0" is 0.5, because the numbers are considered decimal fractions with a non-zero integer and a zero fraction. Mixing integers and floats reverts everything to integers, so "1/2.0" is 0. This lets you write fairly creative math statements in Java, and consequently in Processing, which will generate potentially wildly diffeent results when ported to Processing.js, since JavaScript only knows "numbers". As far as JavaScript is concerned, "2" and "2.0" are the same number, and this can give rise to very interesting bugs when converting a sketch from Processing to JavaScript via Processing.js.
    
    If you were hoping we solved this in some cool and creative way, the resolution of this problem may disappoint; we didn't solve it. Short of adding a symbol table with strong typing so that we can "fake" types in Javascript and switch functionality based on type, this incompatibility could not be solved, and so rather than adding bulk to the code and slowdown to execution, we left this quirk in. It is a well-documented quirk, and "good code" won't try to take advantage of JAva's implicit number type casting, but sometimes you forget, and the result can be quite interesting indeed.
    
    
    JAVA IS CLASS-BASED OBJECT ORIENTED, WITH SEPARATE VARIABLE AND METHOD HEAPS. JAVASCRIPT IS NOT.
    
    JavaScript uses prototype objects, and the inheritance model that comes with it. This means all objects are essentially key/value pairs where each key is either a property (a value, array, or object) or a function, and the typical hierarchical concept of "superclass" and "subclass" don't exist. In order to make "proper" Java-style Object Oriented code work, We had to implement classical inheritance for JavaScript in Processing.js, without making it super slow (we think we succeeded in that respect), and we had to come up with a way to prevent variable and function names from stepping on each other. There, we didn't succeed, as it would have slowed down the library a lot to create separate administration for variables and methods/functions. As such, again the documentation explains that it's a bad idea to use variables and functions with the same name. If everyone wrote "proper" code, this wouldn't be much of a problem, as you want to name vars and functions based on what they're for, or waht they do, but the real world does different things. Sometimes your code won't work, and it's because we decided that code breaking sometimes because your variable name is clobbering your function (or vice versa) is preferable to a really slow program.
    
    
    JAVA ALLOWS METHOD OVERLOADING. JAVASCRIPT DOES NOT.
    
    One of Java's powers is that you can define some function, let's say add(int,int), and then define another function with the same name, but different number of arguments, for instance add(int, int, int), or with different argument types such as add(ComplexNumber, ComplexNumber). Calling add() with two or three integer arguments will automatically call the right function, and calling add() with floats or Car objects will generate an error. JavaScript, on the other hand, does not support this. In Javascript, a function is a property, and you can call it as either a variable (in which case JavaScript tells you whether the property exists or not), or you can call it as a function, using parentheses with zero or more arguments. If you defined a function as add(x,y) and you call it as add(1,2,3,4,5,6), JavaScript is okay with that. It'll set x to 1 and y to 2 and simply ignore the rest. In order to make overloading work, we rewrite functions with the same name but different argument count, so that depending on how many arguments are found for a function call in the sketch source, we rewrite function(a,b,c) in the source to function$3(a,b,c), and function(a,b,c,d) to function$4(a,b,c,d), thus ensuring correct code paths. One thing we couldn't solve was the issue where functions had the same number of arguments, but multiple datatypes. Especially motivated by the impossibility of telling numerical datatypes apart, we decided to simply leave this as an unsolved issue.


    JAVA ALLOWS IMPORTING COMPILED CODE...
    
    Sometimes, plain Processing is not enough. You want additional functionality, and you hit up the internet, download a Processing library, which is a jarchive with compiled java code, drop it in your Processing library dir, and off you go - you now have networking, or audio, or video, or kinect interfacing, or... anything you want that someone wrote a library for.
    
    This is a problem, because compiled java code is JVM bytecode. This has given us many headaches: how do we support library imports without writing Java bytecode decompiler? After about a year of discussions, we settled on what may seem the simplest solution. Rather than trying to also cover Processing libraries, we decided to support the import keyword in sketches, and create a Processing.js Library API, so that library developers can write a javascript version of their library (where feasible, given the web's nature) so that if their javascript library is loaded --say processing.video.js-- then a sketch that uses that library as import --say, "import processing.video"-- simply works. Slated to be part of Processing.js 1.4, import functionality is the last major feature that is missing from Processing.js (we support the import keyword in 1.3.x by completely ignoring it), and will be the last major step towards parity... at least until Processing changes API in some major way again, and we're back to open source step one!


  WHY JAVASCRIPT IF IT CAN'T DO JAVA?

    This is not an unreasonable question, and it has multiple answers. The most obvious one is that JavaScript comes with the browser. You don't "install" JavaScript yourself, there's no plugin to download first, it's just there. If you want to port something to the web, you're stuck with JavaScript. Although given the flexibility of JavaScript, "stuck with" is really not doing justice to how powerful the language is. One reason to pick JavaScript is "because it's already there".
    
    However, the proper answer explains that it's not really true that JavaScript "can't do" the things that Java does; it'd just be slower. Even though out of the box JavaScript can't do some of the things Java does, it's still a proper programming language and it can be made to emulate any other programming language, at the cost of speed. We could, technically, write a full java interpreter, with a String heap, separate variable and method models, class vs. instance Object Orientation with rigid class hierarchies, and everything else under the Sun (or, these days, Oracle), but that's not what we're in it for: Processing.js is about offering a Processing-to-the-web conversion, in as little code as is necessary for that. This means that even though we decided not to make it do certain Java things, our library has one huge benefit: it can cope with JavaScript really, really well.
    
    In fact, during a meeting between the Processing.js and Processing people at Bocoup in Boston, in 2010, Ben Fry asked John Resig about why he used regular expression replacement and only partial conversion instead of doing a proper parser and compiler. John's response was that it was important to him that people be able to mix Processing syntax (Java) and JavaScript without having to choose between them. That initial choice has been crucial in shaping the philosophy of Processing.js ever since. We've worked hard to keep it true in our code, and we can see a clear payoff when we look at all the "purely web" users of Processing.js, who never used Processing, and will happily mix Processing and JavaScript syntax with things working just fine. A lot of things in Java are promisses: strong typing is a content promise to the compiler, visibility is a promise on who will call methods and reference variables, interfaces are promises that instances contain the methods the interface describes, etc. Break those promises and the compiler complains. But, if you don't --and this is a one of the most important thoughts for Processing.js-- then you don't need to formalise those promises in order for the code to work. If you stick a number in a var, and your code treats that var as if it has a number in it, then "var varname" is just as good as "int varname". Do you need typing? In Java, you do; in JavaScript, you don't, so why force it in? The same goes for other code promises. If the Processing compiler doesn't complain about your code, then we can strip all the explicit syntax for your promises and it'll still work the same.
    
    This has made Processing.js a ridiculously useful library for data visualisation, media presentation and even entertainment. Sketches in native Processing work, but sketches that mix Java and JavaScript also work just fine, and sketches that use pure javascript, treating Processing.js as a glorified canvas drawing framework also work just fine. In an effort to reach parity with native Processing, without forcing Java-only syntax, the project had been taken in by an audience as wide as the web itself. We've seen activity all over the web using Processing.js. Everyone from IBM to Google has built visualizations and games with Processing.js. People have even won contests writing Processing-on-the-web solutions. Processing.js is making a difference.
    
    Another great thing about converting Java syntax to Javascript, while leaving JavaScript untouched, is that we've enabled something we hadn't even thought about ourselves: Processing.js will work with anything that will work with JavaScript. One of the really cool things that we're now seeing, for instance, is that people are using CoffeeScript in combination with Processing.js, with really cool results. Even though we set out to build a syntax-aware Processing for the web, people took what we did and used it with brand new syntaxes. They could never have done that if we had made Processing.js simply a java interpreter. By sticking with code conversion rather than writing a code interpreter, Processing has been given a reach on the web far beyond what it would have had if it had stayed "Java" only, even if that was Java only in syntax, with execution on the web taken care of by JavaScript. The upstream acceptance of our code alone has been both amazing and inspiring. Clearly we're doing something right, and the web seems happy with what we're doing.


PROCESSING.JS AS A PROJECT IN TIME

  If you're interested in learning about how various Open Source projects deal with issues, you usually also want an overview of how projects have changed over time. We've already looked at the transition from start-up to (mostly) mature code base, but some history and project statistics are always nice to get a sense of how a project is grounded in reality.

  The project started as an attempt by John Resig at the end of 2007. In May 2008, he had a small proof of concept library that could do some of the things that Processing could do at the time, without the need for java applets on a page. John blogged about this on his website (http://ejohn.org/blog/processingjs), stating "The full source code is contained within a single file. It comes in at about 5000 lines, compresses down to less than 10kb." and releases the source code on github (at http://github.com/jeresig/processing-js/tree/master). This achievements generates enough attention to warrant comments by some influential people and news outlets, with Wired writing: "We cover a lot of language and software developments here at Compiler, but this might be the most impressive thing weâ€™ve ever seen." The project also catches the attention of Mozilla's Chris Blizzard, who hits the nail on the head when he states: "[it is] Easy to drop in graphical interactive elements into other sites with the same transparency and zero-barrier to learning we've seen from the rest of the web. Think about how fast that stuff might spread on the web, how we might end up with people sharing and learning together and how much better the experience on the web might be in the end. That iterative process is one that needs starting points and what John has done is give us a great starting point." 
  
  This attracted the attention of Al McDonald (then working at Bocoup, www.bocoup.com), who joined John as maintainer of Processing.js, turning the project from a one man job into a project. Together they added many of the still missing features, and set up a proper web site with tech demos. In 2009 Yury Delendik joined, who has arguably been the most important contributor to Processing.js given his contribution: writing (and continuing to effectively own to this day) the parser code for Processing.js (over the course of 2009 and 2010 he ends up completely rewriting the parser to suit the ever increasing complexity and coverage of Processing.js with respects to both Processing, as well as plain Java syntax). Then, in May 2009, the project gets a major boost in the form of Mozilla's Chris Blizzard meeting with David Humphrey, an Open Source professor at Seneca College, to discuss the possibility of Seneca working on Processing.js to complete the work that John and Al started. After conversations with Ben Fry, students are set loose on Processing.js in the fall, with Mozilla Foundation's Frank Hecker and David working out a detailed project plan, listing the motivations behind Mozilla's involvement, leading to Mozilla's "Processing for the Web" initiative (https://wiki.mozilla.org/Education/Projects/ProcessingForTheWeb).

  David Humphrey becomes the maintainer of Processing.js, and starts to work on completing the library. An IRC channel (#processing.js) is set up on irc.mozilla.org for development discussions, and students get their first taste of real work open source software development. Project management is moved to Lighthouse (http://processing-js.lighthouseapp.com) and on November 17th, 2009, the first public release of Processing.js since the Seneca community took over the project is made, with a 0.1 version number.
  
  In April of 2010 Ben Fry, John Resig, Al MacDonald, David Humphrey, Corban Brook (who ends up being lead developer and release manager for several releases), and Chris Blizzard all participate in a panel discussion at Bocoup in Boston, focussed on Processing.js. This is the first time the Processing and Processing.js communities meet in person, and many new ideas are thrown around. In a first act of spontaneous web reciprocation, May 2010 sees Ari Bader-Natal mixing Processing.js with Etherpad, creating "Studio Sketchpad" (http://blog.sketchpad.cc/2010/05/welcome), a website that lets people write and share Processing sketches online, rendered using Processing.js, greatly boosting the popularity and use of the project.
  
  Then, finally, on November 18th 2010, Processing.js reaches version 1.0 and is released into the world at large (and in good open source tradition is immediately followed up by 1.1.0, which fixes several critical bugs uncaught in the 1.0 release!).

  After this, the development focus shifts from "reaching parity with Processing" to "refining the codebase", with missing features implemented as people who use Processing.js in the wild discover deficiencies or bugs. Continuing to gain interest from the open source and web communities, the Processing.js team is approached by Greg Wilson of "The Architecture of Open Source Applications" in the summer of 2011 about the possibility of a chapter on Processing.js in the upcoming second volume.
  
  In the mean time, native Processing sees several changes as well, the most important being the addition of the concept of "modes" to the IDE. In June, a JavaScript mode tool for the IDE is released, allowing people to export their code for use on the web, using Processing.js as render library. Having achieved close to functional parity with native Processing, the Processing.js library is finally considered mature enough to move Casey Reas to convert all the examples and demos on the Processing website to use Processing.js in September, 2011, in anticipation of the Processing 2.0 release.
  
  And that brings us to today, with the Processing.js team working hard at adding the last missing bits of functionality, and putting in much effort to squeezing out performance while trimming down the code base in an effort to make Processing.js just that much better. 
  
  So how has the work on Processing.js panned out expressed in numbers and figures? We've had over 25 releases up to september 2011, with a widely varying number of issues fixed per release. What follows is a small glimpse into our development track record so far.
  
  2009-11-19: version 0.1, covering 17 tickets
  2009-11-21:	version 0.2, covering 4 tickets
  2009-12-12:	version 0.3, covering 4 tickets
  2010-01-24:	version 0.4, covering 24 tickets
  2010-02-05:	version 0.5, covering 74 tickets
  2010-02-19:	version 0.6, covering 41 tickets
  2010-03-15:	version 0.7, covering 75 tickets
    (immediately followed by 0.7.1 on the same day to fix a critical bug)
  2010-04-08:	version 0.8, covering 72 tickets
  2010-05-06:	version 0.9.0, covering 86 tickets
  2010-05-12:	version 0.9.1, covering 2 tickets
  2010-06-04:	version 0.9.3, covering 91 tickets
  2010-06-16:	version 0.9.4, covering 6 tickets
  2010-07-16:	version 0.9.6, covering 49 tickets
  2010-07-30:	version 0.9.7, covering 52 tickets
  2010-09-03:	version 0.9.8, covering 27 tickets
  2010-10-28:	version 1.0 is finally released, covering 79 tickets
  2011-02-15:	version 1.1, covering 54 tickets
  2011-03-24:	version 1.1.1, covering 14 tickets
  2011-05-25:	version 1.2.0, covering 107 tickets
  2011-06-02:	version 1.2.1, covering 6 tickets
  2011-07-14:	version 1.2.2, covering 107 tickets
    (followed by 1.2.3 an hour later, which fixes three critical bugs)
  2011-08-25:	version 1.3.0, covering a massive 211 tickets

  In total some 1,200 tickets have been resolved since project management was moved to Lighthouse, with an additional 293 tickets marked as duplicates (108), wont-fix (74) or invalid (111). The average time between releases is roughly 26 days, with a few ourlines (110 days between 1.0 and 1.1, and mere hours between 0.7 and 0.7.1, and 1.2.2 and 1.2.3). The exact numbers for developers and contributors per release are unknown, but the numbers have been as low as three developers (the minimum required to do a two step peer/super review process) and as high as 13 developers for a single release. Processing.js has grown from 10KB to 100KB in version 0.4 to 150KB, 172KB, 215KB, 244KB and 282KB over versions 0.5 through 0.9, and growing from 282KB in 0.9.1 to 411KB in version 1.3.0 with a minified version of 230KB (compressed to 64KB when used in gzipped form).

  With every ticket solved, parser, performange, visual reference and/or unit tests were contributed to the test suite, so that today the full development environment consists of some 1200 parser test cases, 115 performance tests, 345 visual reference tests and 174 unit tests. Any patch has to pass all of these, with special "Make" commands to test patches against each test set separately, or all of them in one run. In addition to this, when a release is built the minified version of Processing.js is also verified separately against our test suite, and the production processing.js file is placed online and run through a battery of visual tests for which we have not yet been able to write either a unit test or automated visual ref test, and only if they pass all tests will the release candidate be packed into a downloadable zip file for public release.

  
CONCLUSION

  Processing.js has seen a long track of development, with many contributors, and even though it can trace it roots all the way back to John Resig's first stab at porting Processing to the web, all the original code has been replaced, sometimes several times over, since 2007. Not just John's code, but many other contributions that made sense at the time when Processing.js was still trying to get near functional parity with native Processing have been replaced by better or faster code, but we couldn't have gotten Processing.js to where it is today without the help of everyone who has helped build out the library over the years. We want to acknowledge the contributions of everyone who has helped keep Processing.js alive, either in a small way or by sticking with the project over multiple releases. In no particular order of importance, John Resig, Alistair MacDonald, David Humphrey, Corban Brook, Anna Sobiepanek, Andor Salga, Daniel Hodgin, Scott Downe, Yuri Delendik, Mike Kamermans, Chris Lonnen, Mickael Medel, Matthew Lam, Jon Buckley, Dominic Baranski, Elijah Grey, Thomas Saunders, Abel Allison, Andrew Grimo, Donghui Liu, Edward Sin, Alex Londono, Robert O'Rourke, Thanh Dao, Zhibin Huang, John Turner, Tom Brown, Minoo Ziaei, Ricard Marxer, Matt Postill, Tiago Moreira, Jonathan Brodsky, and Roger Sodre were all instrumental in realising this project, and with continued contributions from some, and new contributions by others, we intend to keep improving Processing.js until parity has been achieved. To put it in the words of Shaun McWhinnie (@himself) in one of his tweets: "Sat down to *learn* processing.js last night, then found out it can parse my existing pde sketches #jobdone". If your Processing code "just works", we've done our job. And when it doesn't, do drop by and let us know what still needs fixing. There's no better motivation for an open source project development team than people saying they want to use it.
