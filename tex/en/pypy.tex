\begin{aosachapter}{PyPy}{s:pypy}{Benjamin Peterson}

PyPy is a Python implementation and a dynamic language implementation framework.

\begin{aosasect1}{A Little History}

Python is high-level, dynamic programming langauge. It was invented by the Dutch
programmer Guido van Rossum in the late 1980s. Guido's original implementation
is a traditional bytecode interpreter. It is written in C and is consequently
known as CPython. There are now many other Python implementations. Among the
most notable are Jython, which is written in Java and allows for interfacing
with Java code, IronPython, which is written in C\# and interfaces with
Microsoft's .NET framework, and PyPy, the subject of this chapter. CPython is
still the most widely used implementation and the only one to support Python 3,
the next generation of the Python language. This chapter will explain the design
decisions in PyPy that make it different from other Python implementations and
indeed from any other dynamic language implementation.

\end{aosasect1}

\begin{aosasect1}{Overview of PyPy}

PyPy, except for a negligible amount of C stubs, is written completely
in Python. The PyPy source tree contains two major
components. Firstly, there is the Python interpreter, which is the
user-facing Python runtime that people using PyPy as a Python
implementation invoke. The Python interpreter is actually written in a
subset of Python called Restricted Python (abbreviated RPython). The
purpose of writing the Python interpreter in RPython is that it can be
fed to the second major part of PyPy, the RPython translation
toolchain. The RPython translator takes RPython code and converts it
to a choosen lower level language, most commonly C
(\aosafigref{fig:pypy:steps}). This allows PyPy to be a self-hosting
implementation. As we shall see throughout this chapter, the RPython
translator also makes PyPy a general dynamic language implementation
framework.

\aosafigure{../images/pypy/translation_steps.png}{Translation Steps}{fig:pypy:steps}

PyPy's powerful abstractions make it the most flexible Python implementation. It
has nearly 200 configuration options, which vary from selecting different
garbage collector implementations to lazy string objects to parameters of
various translation optimizations.

\end{aosasect1}

\begin{aosasect1}{The Python Interpreter}

Since RPython is a strict subset of Python, the PyPy Python interpreter can be
run on top of another Python implementation untranslated. This is, of course,
extremely slow but it makes it possible to quickly test changes. It also enables
normal Python debugging tools to be used to debug the interpreter. Most of
PyPy's interpreter tests can be run both on the untranslated interpreter and the
translated interpreter. This allows quick testing during development and
assurance that the translated interpreter behaves the same as the untranslated
one.

For the most part, the details of the PyPy Python interpreter are quite similiar
to that of CPython; they use the nearly identical bytecode and data structures
during interpretation. The primary difference between the two is PyPy has a
clever abstraction called \emph{object spaces} (or objspaces for short). An
objspace encapsulates all the knowledge needed to represent and manipulate
Python data types. For example, performing a binary operation on two Python
objects or fetching an attribute is handled completely by the objspace. This
allows the interpreter to not know anything about the implementation details of
Python objects. The interpreter treats Python objects as black boxes and calls
objspace methods whenever it needs to use them. For example, here is a rough
implementation of the \verb+BINARY\_ADD+ opcode, which is called when two
objects are combined with +

\begin{verbatim}
def BINARY_ADD(space):
    object1, object2 = # pop 2 values off the stack
    result = space.add(object1, object2)
    # push result onto the stack
\end{verbatim}

The objspace abstraction has numerous advantages. It allows new data type
implementations to be swapped in and out without modifying the
interpreter. Also, since the sole way to manipulate objects is through objspace,
the objspace can intercept, proxy, or record operations on objects. Using the
powerful abstraction of objspaces, PyPy has experimented with thunking, where
results can be lazily but completely transparently computed on demand, and
tainting, where any operation on an object will raise an exception (useful for
passing sensitive data through untrusted code). The most important application
of objspaces, however, will be discussed in section~\ref{sec:translator}.

The objspace used in a vanilla PyPy interpreter is called the \emph{standard
  objspace} or std objspace for short. In addition to the abstraction provided
by the objspace system, the std objspace provides another level of indirection;
a single data type may have multiple implementations. Operations on data types
are then dispatched using multimethods. This allows picking the most efficient
representation for a given piece of data. For example, the Python long type
(ostensibly a bigint data type) can be represented as a standard machine word
sized integer when it is small enough, so the memory and computationally less
efficient arbitrary-precision implementation need only be used when
necessary. (There's even an implementation of Python integers available using
tagged pointers.) Container types can also be specialized to certain data
types. For example, PyPy has an dictionary (Python's hash table data type)
implementation specialized for strings keys. The fact that the same data type
can be represented by different implementations is completely transparent to
application level code; a dictionary specialized to strings is identical to a
generic dictionary and will degenerate gracefully if non-string keys are put
into it.

PyPy distinguishes between interpreter-level (interp-level) and
application-level (app-level) code. Interp-level code, what most of the
interpreter is written in, must be in RPython and is translated. It directly
works with the objspace and wrapped Python objects. App-level code is always run
by the PyPy bytecode interpreter. As simple as interp-level RPython code is
compared C or Java, PyPy developers have found it easiest to use pure app-level
code for some parts of the interpreter. Consequently, PyPy has support for
embedding app-level code in the interpreter. For example, the functionality of
the Python \verb+print+ statement is implemented in app-level Python. Builtin-in
modules can also be written partially in interp-level code and partially in
app-level code.

\end{aosasect1}

\begin{aosasect1}{The RPython Translator}
\label{sec:translator}

The RPython translator is really a toolchain of several lowering phases that
reduce RPython to a target language, typically C. It is itself written in
(unrestricted) Python and intimately linked to the PyPy Python interpreter for
reasons that will be illuminated shortly.

The first thing the translator does is load the RPython program into its
process. (This is done with the normal Python module loading support.) RPython
imposes a set of restrictions on normal, dynamic Python. For example, functions
cannot be created at runtime, and a single variable cannot have the possibility
of holding incompatible types, such as an integer and a object instance. When
the program is loaded by the translator, though, it is running on a normal
Python interpreter and can use all of Python's dynamic features. PyPy's Python
interpreter, a huge RPython program, makes heavy use of this feature. For
example, it generates code for standard objspace multimethod dispatch. The only
requirement is that the program is valid RPython by the time the translator
starts the next phase of translation.

The translator builds flow graphs of the RPython program through a process
called \emph{abstract interpretation}. Abstract interpretation reuses the PyPy
Python interpreter to interpret RPython programs in a special objspace called
the \emph{flow objspace}. Recall that the Python interpreter treats objects in a
program like black boxes, calling out to the objspace to perform any
operation. The flow objspace, instead of the standard set of Python objects, has
only two objects: variables and constants. Variables represent values not known
during translation, and constants, obviously, immutable values that are
known. What is immutable and must be constant in RPython is broader than in
standard Python. For example, modules, which are emphatically mutable in Python,
are constants in the flow objspace because they don't exist in RPython and must
be constant-folded out. As the Python interpreter interprets the bytecode of
RPython functions, the flow objspace records the operations it is asked to
perform. It also takes care to record all branches of conditional control flow
constructs. The end result of abstract interpretation for a function is a
flow-graph consisting of linked blocks.

An example of the flow-graph generating process is in order. Consider a simple
factorial function

\begin{verbatim}
def factorial(n):
    if n == 1:
        return 1
    return n * factorial(n - 1)
\end{verbatim}

The flow-graph for the function looks like \aosafigref{fig.pypy.flowgraph}

\aosafigure{../images/pypy/flowgraph.png}{Flow-graph of factorial}{fig.pypy.flowgraph}

As you can see, the factorial function has been divided into blocks containing
the operations the flowspace recorded. Each block has input arguments and a list
of operations on variables and constants. At the end of the block is an exit
switch, which determines which block control flow will pass to based on the
value of some variable.

The flow-graph generated in the flow objspace is in static single assignment
form, or SSA, an intermediate representation commonly used in compilers. The key
feature of SSA is that every variable is only assigned once.

After a function graph is generated, the annotation phase beings. The annotator
assigns a type to the result of each SSA operation. The factorial function above
will be annotated to accept and return an integer.

The next phase is called RTyping. RTyping uses type information from the
annotator to expand each high-level flow-graph operation into low-level ones. It
is the first part of translation that the target backend matters. The RTyper
currently has two type systems: A low level typesystem for backends like C and
one for higher level typesystems with classes. The backend chooses which type
system the program is specialized to. High-level Python operations and types are
transformed into the level of the type system. For example, for an +add+
operation with operands annotated as integers will generate an +int\_add+ with
the low-level type system. More complication operations like hash table lookups
generate function calls.

After RTyping, some optimizations on the low-level flow-graph are
performed. They are mostly of the traditional compiler variety like constant
folding, store sinking, and dead code removal. Python code in general makes
frequent dynamic memory allocations. RPython, begin a Python derivative,
inherits this allocation intensive pattern. In many cases, though, allocations
are temporary and local to a function. \emph{Malloc removal} is an optimization
that addresses these cases. Malloc removal removes these allocations by
``flattening'' the previously dynamically-allocated object into component
scalars when possible.

To see how malloc removals works, consider the following function that computes
the euclidean distance between two points on the plane in a roundabout fashion
\begin{verbatim}
def distance(x1, y1, x2, y2):
    p1 = (x1, y1)
    p2 = (x2, y2)
    return math.hypot(p1[0] - p2[0], p1[1] - p2[1])
\end{verbatim}

When initially RTyped, the body of the function has the following operations:
\begin{verbatim}
v60 = malloc((GcStruct tuple2))
v61 = setfield(v60, ('item0'), x1_1)
v62 = setfield(v60, ('item1'), y1_1)
v63 = malloc((GcStruct tuple2))
v64 = setfield(v63, ('item0'), x2_1)
v65 = setfield(v63, ('item1'), y2_1)
v66 = getfield(v60, ('item0'))
v67 = getfield(v63, ('item0'))
v68 = int_sub(v66, v67)
v69 = getfield(v60, ('item1'))
v70 = getfield(v63, ('item1'))
v71 = int_sub(v69, v70)
v72 = cast_int_to_float(v68)
v73 = cast_int_to_float(v71)
v74 = direct_call(math_hypot, v72, v73)
\end{verbatim}
This code is suboptimal in several ways. Two tuples that never escape the
function are allocated. Additionally, there is unnecessary indirection accessing
its fields.

Running malloc removal produces the following consise code:
\begin{verbatim}
v53 = int_sub(x1_0, x2_0)
v56 = int_sub(y1_0, y2_0)
v57 = cast_int_to_float(v53)
v58 = cast_int_to_float(v56)
v59 = direct_call((<* fn ll_math.ll_math_hypot>), v57, v58)
\end{verbatim}
The tuple allocations have been completely removed and the indirections
flattened out. Later, we will see how a technique similar to malloc removal is
used on application level Python in the PyPy JIT in section~\ref{sec:jit}.

PyPy also does function inlining. Inlining improves performance and reduces the
size of the final binary because it allows more constant folding and malloc
removal to take place.

The program, now in optimized, low-level flow-graphs, is passed to the backend
to generate sources. Before it can generate C code, the C backend must perform
some additional transformations. One of these is exception transformation, where
exception handling is rewritten to use manual stack-unwinding. Another is the
insertion of stack depth checks. These raise an exception at runtime if the
recursion is too deep.

Another one of the transformations performed by the C backend is adding garbage
collection. RPython is a garabage collected language, but C is not, so a garbage
collector has to be added. To do this, a garbage collection transformer converts
the flow-graphs of the program into a garbage collected program. PyPy's GC
transformers provide an excellent demonstration of how translation abstracts
away mundane details. In CPython, which uses reference counting, the C code of
the interpreter must carefully keep track of references to Python objects it is
manipulating. This not only hardcodes the garbage collection scheme in the
entire codebase but is prone to subtle human errors. PyPy's GC transformer
solves both problems. It allows different garbage collection schemes to be
swapped in and out seamlessly. It is trivial to evaluate a garbage collector
implementation (of which PyPy has many), simply by tweaking an option at
translation. Modulo transformer bugs, it also never makes reference mistakes or
forgets to inform the GC when an object is no longer in use. The power of the GC
abstraction allows GC implementations that would be practically impossible to
hardcode in an interpreter. For example, most of PyPy's GC implementations
require a \emph{write barrier}. A write barrier is a check which must be
performed every time a GC managed object is placed in another GC-managed array
or struct. The process of inserting write barriers would be laborious and
fraught with mistakes if done manually but is trivial when done automatically by
the GC transformer.

Finally, the C backend emits C source code. The generated C code, being a code
representation of a low-level flow-graph, is an ugly mess of gotos and obscurely
named variables. An advantage of writing C is that the C compiler can do most of
the complicated static transformation work like loop optimizations and register
allocation.

\end{aosasect1}

\begin{aosasect1}{The PyPy JIT}
\label{sec:jit}

Python, like most dynamic languages, has traditionally traded efficiency for
flexibility. The architecture of PyPy, being especially rich in flexibility and
abstraction, makes very fast interpretation difficult. The powerful objspace and
multimethod abstractions do not come without a cost. Consequently, the vanilla
PyPy interpreter performs up to 4 times than slower CPython. To remedy not only
this but Python's reputation as a slow language, PyPy has a Just-in-time
compiler (commonly just JIT). The JIT compiles frequently used codepaths to
assembly during the runtime of the program.

The PyPy JIT takes advantage PyPy's unique translation architecture described
above. PyPy actually has no \emph{Python-specific} JIT; it has JIT
generator. JIT generation is implemented as simply another optional pass during
translation. A interpreter desiring JIT generation need only make two special
function calls called ``hints''.

PyPy's JIT is a \emph{tracing JIT}. This means it detects ``hot'' (meaning
frequently run) loops to optimize by compiling to assembly. When the JIT has
decided it is going to compile a loop, it records operations in one iteration of
the loop, a process \emph{tracing}. These operations are subsequently compiled
to machine code.

As mentioned above, the JIT generator requires only two hints in the interpreter
to generate a JIT: \verb+merge_point+ and
\verb+can_enter_jit+. \verb+can_enter_jit+ unsurprisingly tells the JIT where in
the interpreter a loop starts. In the Python interpreter, this is the end of the
\verb+JUMP_ABSOLUTE+ bytecode, which occurs at the end of Python \verb+while+
and \verb+for+ loops. \verb+merge_point+ tells the JIT where it is safe to
return to the interpreter from the JIT. This is the beginning of bytecode
dispatch in the Python interpreter.

The JIT generator is invoked after the RTyping phase of translation. Recall that
at this point, the program's flow-graphs consist of low-level operations nearly
ready for target code generation. The JIT generator locates the hints mentioned
above in the interpreter and replaces them with calls to invoke the JIT during
runtime. The JIT generator then writes a serialized representation of the
flow-graph called a jitcode for every function that the interpreter wants
jitted. The entire interpreter is now described in terms of low-level RPython
operations. The jitcodes are saved in the final binary for use at runtime.

At runtime, the JIT maintains a counter for every loop that is executed in the
program. When the loop counter exceeds a configurable threshold, the JIT is
invoked and tracing begins. The key concept in tracing is the
\emph{meta-interpreter}. The meta-interpreter executes the jitcodes created in
translation. It is thus interpreting the main interpreter, hence the name. As it
traces the loop, it creates a list of the operations it is executing and records
them in JIT intermediate representation (IR), another instruction format. The
meta-interpreter is forced to specialize the trace to properties of the current
iteration. For example, when the meta-interpreter encounters a conditional in
the jitcode, it naturally must choose one path. When it makes a choice based on
runtime information, the meta-interpreter records an IR operation called a
\emph{guard}. In the case of a conditional, this will be a \verb+guard_true+ or
\verb+guard_false+ operation on the condition variable. Most arithmetic
operations also have guards, which ensure the operation did not
overflow. Essentially, guards codify assumptions the meta-interpreter is making
as it traces. When assembly is generated, the guards will protect assembly from
being run in an invalid context. Tracing ends when the meta-interpreter reaches
the same \verb+can_enter_jit+ operation with which it started tracing. The loop
IR can now be passed to the optimizer.

The JIT optimizer features a few classical compiler optimizations and many
optimizations specialized for dynamic languages. Among the most important of the
latter are \emph{virtuals} and \emph{virtualizables}. Virtuals are objects which
are known not to escape the trace, meaning they are not passed as arguments to
an external function call. Structs and constant length arrays can be
virtuals. Virtuals do not have to be allocated, and their data can be stored
directly in registers and on the stack. (This is much like the static malloc
removal phase described in the paragraph about translation backend
optimizations.) The virtuals optimization strips away the indirection and memory
allocation inefficiencies in the Python interpreter. For example, boxed Python
integer objects are unboxed into simple word-sized integers by becoming
virtual. A virtualizable acts much like a virtual but may escape the trace. In
the Python interpreter, the frame object, which holds variable values and the
instruction pointer, is marked virtualizable. This allows stack manipulations
and other operations on the frame to be optimized out. Although virtuals and
virtualizables are similar, they share nothing in implementation. Virtualizables
are handled during tracing not trace optimization like virtuals. The reason for
this is virtualizables require special treatment, since they may escape the
trace. During JIT generation, code which accesses a virtualizable is rewritten
to check if JITed assembly is running. If it is, the JIT is asked to update the
fields from data in assembly. Additionally when the external call returns to
JITed code, execution bails back to the interpreter (\aosafigref{fig:pypy:bail}).

\aosafigure{../images/pypy/guard_fails.png}{Bailing Back to the Interpreter}{fig:pypy:bail}

When a guard fails, the compiled assembly is now longer valid, and control must
return to the bytecode interpreter. This bailing out is one of the most
difficult parts of JIT implementation, since the interpreter state has to be
reconstructed from the register and stack state at the point the guard
failed. The failing guard may be in the middle of the execution of a complicated
opcode, so the interpreter can not just start with the next opcode. To solve
this, PyPy uses a \emph{blackhole interpreter}. The blackhole interpreter
executes jitcodes starting from the point of guard failure until the next merge
point is reached at which point the real interpreter can resume. Its name comes
from the fact that unlike the meta-interpreter, it doesn't record any of the
operations it executes.

Since the JIT IR is already quite low-level, actual assembly generation is not
too difficult. Most operations correspond with only a few x86 assembly
operations. The register allocator is simple linear algorithm. At the moment,
the increased time that would be spent in the backend with a more sophisticated
register allocation algorithm in exchange for generating slightly better code
has not been justified. The trickiest portions of assembly generation are
garbage collector integration and guard recovery. The GC has to be made aware of
stack roots in the generated JIT code. This is accomplished by special support
in the GC for dynamic root maps. For each guard, a compact description of where
all the values needed to reconstruct state is written. At guard failure,
execution jumps to a function which decodes this description and passes the
recovery values to a higher level be reconstructed.

How successful have the techniques used in the PyPy JIT proven? At the time of
this writing, PyPy is an (geometric) average of 5 times faster than CPython on a
comprehensive suite of benchmarks. With the JIT, app-level Python has the
possibility of being faster than interp-level code. PyPy developers have
recently had the excellent problem of having to write interp-level loops in
app-level Python for performance.

Most importantly, the fact that the JIT is not specific to Python means it could
be applied to any language interpreter written within the Python framework. It
is already used for Python's regular expression engine. NumPy is a powerful
array module for Python used in numerical computing and scientific
research. PyPy has an experimental reimplementation of NumPy. It harnesses the
power of the PyPy JIT to speed up operations on arrays. While the numpy
implementation is still in its early stages early performance results look
promising.

\end{aosasect1}

\begin{aosasect1}{Design Drawbacks}
\label{sec:drawbacks}

While it beats C any day, writing in RPython can be a frustrating
experience. Its implicit typing is difficult to get used to at first. Not all
Python language featuress are supported and others are arbitrarily
restricted. RPython is not specified formally anywhere and what the translator
accepts can vary from day to day as RPython is adapted to PyPy's needs. The
author often manages to create untranslatable programs that translate for half
an hour only to spit out an obscure error.

The fact the RPython translator is a whole-program analyzer creates some
practical problems. The smallest change anywhere in translated code requires
retranslating the entire interpreter. That currently takes about 40 mintues on a
fast modern system. The delay is especially annoying for testing how changes
affect the JIT, since measuring performance requires a translated
interpreter. The requirement that the whole program be present at translation
means modules containing RPython cannot be built and loaded separately from the
core interpreter.

The levels of abstraction in PyPy are not always as clear cut as in
theory. While technically the JIT generator should be able to produce an
excellent JIT for a language with only the two hints mentioned above, the
reality is that it behaves better on some code than others. The Python
interpreter has seen a lot of work towards making it more ``jit-friendly''
including many more JIT hints and even new data structures optimized for the
JIT.

The many layers of PyPy can make tracking down bugs a laborious process. A
Python interpreter bug could be directly in the interpreter source or buried
somewhere in the semantics of RPython and the translation toolchain. Especially
when a bug cannot be reproduced on the untranslated interpreter, debugging is
difficult. It typically involves running GDB on the nearly unreadable generated
C sources.

Translating even a restricted subset of Python to a much lower level language
like C is not an easy task. The lowering passes described in
section~\ref{sec:translator} are not really independent. Functions are being
annotated and rtyped throughout translation, and the annotator has some
knowledge of low-level types. The RPython translator is thus a tangled web of
cross-dependencies. The translator could do with cleaning up in several places,
but doing it is neither easy nor much fun.

\end{aosasect1}

\begin{aosasect1}{A Note on Process}

In part to combat the complexity of PyPy (see section~\ref{sec:drawbacks}), PyPy
has adopted several so-called ``agile'' development methodologies. By far the
most important of these is test-driven development. All new features and bug
fixes are required to have a test to verify their correctness. The PyPy Python
interpreter is also run against CPython's regression test suite. PyPy's test
driver, py.test, was spun off and is now used in many other projects. PyPy also
has a continuous integration system that runs the test suite and translates the
interpreter on a variety of platforms. Binaries for all platforms are produced
daily and the benchmark suite is run. All these tests ensure that the various
components are behaving no matter what change is made in the complicated
architecture.

There is a strong culture of experimentation in the PyPy project. Developers are
encouraged to make branches in the Mercurial repository. There, ideas in
development can be refined without destabilizing the main branch. Branches are
not always successful, and some are abandoned. If anything though, PyPy
developers are tenacious. Most famously, the current PyPy JIT is the \emph{fith}
attempt to add a JIT to PyPy!

The PyPy project also prides itself on its visualization tools. The flow-graph
charts in section~\ref{sec:translator} are one example. PyPy also has tools to,
for example, show invocation of the garbage collector over time and regular
expression parse trees. Of special interest is jitviewer, a program allows that
one to visually peel pack the layers on a JITed function, from Python bytecode
to JIT IR to assembly. Visualization tools help developers understand how PyPy's
many layers interact with each other.

\aosafigure{../images/pypy/jitviewer.png}{The jitviewer showing Python bytecode
  and associated JIT IR operations}{fig.pypy.jitviewer}

\end{aosasect1}

\begin{aosasect1}{Summary}

The Python interpreter treats Python objects as black boxes and leaves all
behavior to be defined by the objspace. Individual objspaces can provide special
extended behavior to Python objects. The objspace approach also enables the
abstract interpretation technique used in translation.

The RPython translator allows details like garbage collection and exception
handling to be abstracted from the language interpreter. It also opens up the
possibly of running PyPy on many different platforms by using different
backends.

One of the most important uses of the translation architecture is the JIT
generator. The generality of the JIT generator allows JITs for new languages and
sub-langauges like regular expressions to be added. PyPy is the fastest Python
implementation today because of its JIT generator.

While most of PyPy's development effort has gone into the Python interpreter,
PyPy can be used for the implementation of any dynamic language. Over the years,
partial interpreters for JavaScript, Prolog, Scheme, and IO have been written
with PyPy.

\end{aosasect1}

\begin{aosasect1}{Lessons Learned}

Finally, some of lessons to take away from the PyPy project:

Repeated refactoring is often a necessary process. For example, it was
originally envisioned that the C backend for the translator would be able to
work off of the highlevel flow graphs! It took several iterations for the
current multi-phase translation process to be born.

The most important lesson of PyPy is the power of abstraction. In PyPy,
abstractions separate implementation concerns. For example, RPython's automatic
garbage collection allow a developer working the interpreter to not worry about
memory management. At the same time, abstractions have a mental cost. Working on
the translation chain involves juggling the various phases of translation at
once in one's head. What layer a bug resides in can also be clouded by
abstractions; abstraction leakage, where swapping components low-level
components that should be interchangable breaks higher-level code, is perennial
problem. More concretely, abstractions can slow a program down by creating too
much indirection.

The flexibility of (R)Python as an implementation language makes experimenting
with new Python language features (or even new languages) easy. Because of its
unique architecture, PyPy will play a large role in the future of Python and
dynamic language implementation.

\end{aosasect1}

\end{aosachapter}
