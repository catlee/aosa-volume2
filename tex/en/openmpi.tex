\begin{aosachapter}{Open MPI}{s:openmpi}{Jeffrey M.\ Squyres}

\begin{aosasect1}{Background}

Open MPI~\cite{gabriel04:_open_mpi} is an open source software
implementation of The Message Passing Interface (MPI) standard.
%
Before the architecture and innards of Open MPI will make any sense,
a little background on the MPI standard must be discussed.

%--------------------------------------------------------------------------

\begin{aosasect2}{The Message Passing Interface (MPI)}

The MPI standard is defined by the MPI
Forum\footnote{\url{http://www.mpi-forum.org/}}; an open group
consisting of parallel computing experts from both industry and
academia.
%
MPI defines an API that is used for a specific type of portable,
high-performance inter-process communication (IPC): {\em message
  passing}.  Specifically, MPI defines reliable transfer of discrete,
typed messages between MPI processes.
%
Although the definition of an ``MPI process'' is subject to
interpretation on a given platform, it usually corresponds to the
operating system's concept of a process (e.g., a POSIX process).
%
MPI is specifically intended to be implemented as middleware, meaning
that upper-level applications call MPI functions to perform message
passing.

MPI defines a high-level API, meaning that it provides abstraction
away from whatever underlying transport is actually used to pass
messages between processes.  The idea is that sending process $X$ can
effectively say ``take this array of 1,073 double precision values and
send them to process $Y$.''  The corresponding receiving process $Y$
effectively says ``receive an array of 1,073 double precision values
from process $X$.''  A miracle occurs, and the array of 1,073 double
precision values arrives in $Y$'s waiting buffer.

Notice what is absent in this exchange: there is no concept of a
connection occurring, no stream of bytes to interpret, and no network
addresses exchanged.  MPI abstracts all of that away, not only to hide
such complexity from the upper-level application, but also to make the
application portable across different environments and underlying
message-passing transports.  Specifically, a correct MPI application
is source compatible across a wide variety of platforms and networks.

MPI defines not only point-to-point communication (e.g., send and
receive), it also defines other communication patterns, such as {\em
  collective} communication.  Collective operations are where multiple
processes are involved in a single communication action.  Reliable
broadcast, for example, is where one process has a message at the
beginning of the operation. At the end of the operation, all processes
in a group have the message.

MPI defines other concepts and communications patterns that are not
described here.\footnote{As of this writing, the most recent version
  of the MPI standard is MPI-2.2~\cite{mpi-2.2}.  Multiple draft
  versions of the upcoming MPI-3 standard have been published; it may
  be finalized as early as late 2012.}

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Uses of MPI}

There are many implementations of the MPI standard that support a
wide variety of platforms, operating systems, and network types.
Some implementations are open source, some are closed source.  
%
Open MPI, as its name implies, is one of the open source
implementations.
%
Typical MPI transport networks include (but are not limited to):
various protocols over Ethernet (e.g., TCP, iWARP, UDP, raw Ethernet
frames, etc.), shared memory, and InfiniBand.

MPI implementations are typically used in so-called ``high performance
computing'' (HPC) environments.  MPI essentially provides the IPC for
simulation codes, computational algorithms, and other ``big number
crunching'' types of applications.  The input data sets on which these
codes operate typically represent too much computational work for just
one server; MPI jobs are spread out across tens, hundreds, or even
thousands of servers, all working in concert to solve one
computational problem.

That is, the applications using MPI are both parallel in nature and
highly compute-intensive.  It is not unusual for all the processor
cores in an MPI job to to run at 100\% utilization.  To be clear, MPI
jobs typically run in dedicated environments where the MPI processes
are the {\em only} application running on the machine (in addition to
bare-bones operating system functionality, of course).

As such, MPI implementations are typically focused on providing
extremely high performance, measured in metrics such as:

\begin{aosaitemize}
\item Extremely low latency for short message passing.  As an example,
  a 1-byte message can be sent from a user-level Linux process on one
  server, through an InfiniBand switch, and received at the target
  user-level Linux process on a different server in a little over 1
  microsecond.
\item Extremely high message network injection rate for short
  messages.  As an example, MPI can inject JMS NEED TO CITE...
\item Quick ramp-up (as a function of message size) to the maximum
  bandwidth supported by the underlying transport.
\item Low resource utilization.  All resources used by MPI (e.g.,
  memory, cache, and bus bandwidth) cannot be used by the application.
  MPI implementations therefore try to maintain a balance of low
  resource utilization while still providing high performance.
\end{aosaitemize}

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Open MPI}

The first version of the MPI standard, MPI-1.0, was published in
1994~\cite{mpi_forum93:_mpi}.  
%
MPI-2.0, a set of additions on top of MPI-1, was completed in
1996~\cite{geist96:_mpi2_lyon}.

In the first decade after MPI-1 was published, a variety of MPI
implementations sprung up.  Many were provided by vendors for their
proprietary network interconnects.  Many other implementations arose
from the research and academic communities.  Such implementations were
typically ``research-quality,'' meaning that their purpose was to
investigate various high-performance networking concepts and provide
proofs-of-concept of their work.  However, some were high enough
quality that they gained popularity and a number of users.

Open MPI represents the union of four research/academic, open source
MPI implementations: LAM/MPI, LA/MPI (Los Alamos MPI), and FT-MPI
(Fault-Tolerant MPI).
%
The members of the PACX-MPI team joined the Open MPI group shortly
after its inception.

The members of these four development teams decided to collaborate
when we had the collective realization that, aside from minor
differences in optimizations and features, our software code bases
were quite similar.  Each of the four code bases had their own
strengths and weaknesses, but on the whole, they more-or-less did the
same things.  So why compete?  Why not pool our resources, work
together, and make an {\em even better} MPI implementation?

After {\em much} discussion, the decision was made to abandon our four
existing code bases and take only the best {\em ideas} from the prior
projects.  This decision was mainly predicated upon the following
premises:

\begin{aosaitemize}
\item Even though many of the inner algorithms and techniques were
  similar among the four code bases, they each had radically different
  implementation architectures, and would be incredible difficult (if
  not impossible) to merge.
\item Each of the four also had their own (significant) strengths and
  (significant) weaknesses.  Specifically, there were features and
  architecture decisions from each of the four that were desirable to
  carry forward.  Likewise, there were poorly optimized and badly
  designed code in each of the four that were desirable to leave
  behind.
\item The members of the four developer groups had also not worked
  directly together before.  Starting with an entirely new code base
  (rather than advancing one of the existing code bases) put all
  developers on equal ground.
\end{aosaitemize}

Thus, Open MPI was born.  Its first Subversion commit was on November
22, 2003.

\end{aosasect2}

\end{aosasect1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{aosasect1}{Architecture}

For a variety of reasons (mostly related to either performance or
portability), C and C++ were the only two possibilities for the
primary implementation language.  
%
C++ was eventually discarded because different C++ compilers tend to
lay out structs/classes in memory according to different optimization
algorithms, leading to different on-the-wire network representations.
%
C was therefore chosen as the primary implementation language, which
influenced several architectural design decisions.

When Open MPI was started, we knew that it would be a large, complex
code base:

\begin{aosaitemize}
\item In 2003, the current version of the MPI standard, MPI-2.0,
  defined over 300 API functions.
\item Each of the four prior projects were large in themselves.  For
  example, LAM/MPI had over 1,900 files of source code, comprising
  over 300,000 lines of code (including comments and blanks).
\item We wanted Open MPI to support more features, environments, and
  networks than all four prior projects put together.
\end{aosaitemize}

We therefore spent a good deal of time designing an architecture that
focused on three things:

\begin{aosaenumerate}
\item Grouping similar functionality together in multiple abstraction
  layers
\item Using run-time loadable plugins and run-time parameters to
  choose between multiple different implementations of the same
  behavior
\item Not allowing abstraction to get in the way of performance
\end{aosaenumerate}

%--------------------------------------------------------------------------

\begin{aosasect2}{Abstraction Layer Architecture}

\aosafigure{../images/openmpi/open-mpi-layers.pdf}{High level
  architectural view of Open MPI showing its three main abstraction
  layers: OPAL, ORTE, and OMPI.}{fig.openmpi.layers}

Open MPI has three main abstraction layers, shown in
\aosafigref{fig.openmpi.layers}:

\begin{aosaitemize}
\item {\em Open, Portable Access Layer (OPAL)}: OPAL is the bottom
  layer of Open MPI's abstractions.  Its abstractions are focused on
  individual processes (vs.\ parallel jobs).  It provides utility and
  glue code such as generic linked lists, string manipulation,
  debugging controls, and other mundane -- yet necessary --
  functionality.

  OPAL also provides Open MPI's core portability between different
  operating systems, such as discovering IP interfaces, sharing
  memory between processes on the same server, processor and memory
  affinity, high-precision timers, etc.

\item {\em Open MPI Run-time Environment (ORTE)}\footnote{Pronounced
    ``or-tay.''}: An MPI implementation must provide not only the
  required message passing API, but also an accompanying run-time
  system to launch, monitor, and kill parallel jobs.  In Open MPI's
  case, a parallel job is comprised of one or more processes that may
  span multiple operating system instances, and are bound together to
  act as a single, cohesive unit.

  In simple environments with little or no distributed computational
  support, ORTE uses {\tt rsh} or {\tt ssh} to launch the individual
  processes in parallel jobs.  More advanced, HPC-dedicated
  environments typically have schedulers and resource managers for
  fairly sharing computational resources between many users.  Such
  environments usually provide specialized APIs to launch and regulate
  processes on compute servers.  ORTE supports a wide variety of such
  managed environments, such as (but not limited to): Torque / PBS
  Pro, SLURM, Oracle Grid Engine, and LSF.

\item {\em Open MPI}: The MPI layer is the highest abstraction layer,
  and is the only one exposed to applications.  The MPI API is
  implemented in this layer, as are all the message passing semantics
  defined by the MPI standard.

  Since portability is a primary requirement, the MPI layer supports a
  wide variety of network types and underlying protocols.  Some
  networks are similar in their underlying characteristics and
  abstractions; some are not.
\end{aosaitemize}

Although each abstraction is layered on top of the one below it, for
performance reasons, the ORTE and OMPI layers can bypass the
underlying abstraction layers and interact directly with the operating
system and/or hardware when needed (as depicted in
\aosafigref{fig.openmpi.layers}).  For example, the OMPI layer uses
OS-bypass methods to communicate with certain types of NIC hardware to
obtain maximum networking performance.

Each layer is built into a standalone library.  The ORTE library
depends on the OPAL library; the OMPI library depends on the ORTE
library.
%
Separating the layers into their own libraries has acted as a
wonderful tool for preventing abstraction violations.
%
Specifically, applications will fail to link if one layer incorrectly
attempts to use a symbol in a higher layer.
%
Over the years, this abstraction enforcement mechanism has saved many
developers from inadvertently blurring the lines between the three
layers.

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Plugin Architecture}

% This section to describe the MCA and how each of the 3 layers has
% the same layout (frameworks and plugins).

Although the initial members of the Open MPI collaboration shared a
similar core goal (produce a portable, high-performance
implementation of the MPI standard), our organizational backgrounds,
opinions, and agendas were -- and still are -- wildly different.
%
We therefore spent a considerable amount of time designing an
architecture that would allow us to be different -- even while sharing
a common code base.

Run-time loadable {\em components} were a natural choice (a.k.a.,
dynamic shared objects, or ``DSOs'', or ``plugins'').  Components
enforce a common API, but place few limitations on the implementation
of that API.
%
Specifically: the same interface behavior can be implemented multiple
different ways.
%
Indeed, utilizing run-time loadable objects allows third-party plugins
to be shipped outside of the core Open MPI package.  Allowing
arbitrary extensibility is quite a liberating policy, both within the
immediate set of Open MPI developers and in the greater Open MPI
community.  For example, third parties can develop and independently
distribute their own Open MPI plugins outside of the core Open MPI
package.

This run-time flexibility is a key component of the Open MPI design
philosophy, and is integrated deeply throughout the architecture.
%
Case in point: the Open MPI v1.5 series includes 155 plugins.
%
There are plugins for different {\tt memcpy()} implementations,
plugins for how to launch processes on remote servers, and plugins for
how to communicate on different types of underlying networks.

One of the major benefits of using using plugins is that multiple
groups of developers have freedom to experiment with alternate
implementations without affecting the core of Open MPI.
%
As such, this component concept is utilized heavily throughout all
three layers of Open MPI -- in each layer, there are many different
{\em types} of components.  
%
Each type of component is enclosed in a {\em framework}.
%
A component belongs to exactly one framework, and a framework supports
exactly one kind of component.
%
\aosafigref{fig.openmpi.mca} shows the architectural layout of the
Open MPI core, its frameworks, and the components that they contain.
%
Open MPI's set of layers, frameworks, and components is referred to as
the Modular Component Architecture (MCA).

\aosafigure{../images/openmpi/open-mpi-mca.pdf}{High level
  architectural view of Open MPI showing frameworks and their bases
  and components.  Each framework contains a base an one or more
  components.  This structure is replicated in each of the layers
  shown in \aosafigref{fig.openmpi.layers}.}{fig.openmpi.mca}

Finally, another major advantage of using frameworks and components is
their inherent composability: using plugin $A$ in framework $B$ has no
bearing on using plugin $X$ in framework $Y$.

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Plugin Frameworks}

Each framework is fully self-contained in its own subdirectory in the
Open MPI source code tree.  The name of the subdirectory is the same
name as the framework; for example, the {\tt memory} framework is in
the {\tt memory} directory.  
%
Framework directories contain at least the following three items:

\begin{aosaenumerate}
\item {\em Component interface definition:} A header file named {\tt
    <framework>.h} will be located in the top-level framework
  directory (e.g., {\tt memory.h}).  This well-known header file
  defines the interfaces that each component in the framework must
  support.  This header includes function pointer typedefs for the
  interface functions, structs for marshaling these function pointers,
  and any other necessary types, attribute fields, macros,
  declarations, etc.

\item {\em Base code}: The {\tt base} subdirectory contains the glue
  code that provides the core functionality of the framework.  The
  base is typically comprised of logistical grunt work such as finding
  and opening components at run-time, common utility functionality
  that may be utilized by plugins, etc.

\item {\em Components}: All other subdirectories in the framework
  directory are assumed to be components.  Just like the framework,
  the names of the components are the same names as their
  subdirectories (e.g., the {\tt posix} subdirectory contains the {\tt
    posix} component).
\end{aosaenumerate}

Just like each framework defines the interfaces to which its
components must adhere, frameworks also define their own rules about
how they bootstrap themselves, how they pick components to use, and
how they are shut down.  Two common examples of how frameworks differ
in their rules / setup are many-of-many vs.\ one-of-many frameworks,
and static vs.\ dynamic frameworks.

%%%%%

\paragraph{Many-of-many frameworks.} 

Some frameworks have functionality that can be implemented multiple
different ways in the same process.
%
For example, Open MPI's point-to-point network framework will load
multiple driver plugins to support different network types in a single
process.

Such frameworks will typically open all components that they can find
and then query each component, effectively asking, ``Do you want to
run?''  
%
The components determine whether they want to run by examining the
system on which they are running.
%
For example, a point-to-point network component will look to see if
the network type it supports is both available and active on the
system.
%
If it is not, it will reply ``No, I do not want to run,'' causing the
framework to close/unload that component.
% 
If that network type {\em is} available, it will reply ``Yes, I want to
run,'' causing the framework to keep the component open for further
use.

%%%%%

\paragraph{One-of-many frameworks.} 

Other frameworks provide functionality that makes sense to have only
one implementation available at run-time.
%
For example, creating a consistent checkpoint of a parallel job --
meaning that the job is effectively ``frozen'' and can be arbitrarily
resumed later -- must be performed using the same back-end
checkpointing system for each process in the job.
%
The one plugin that interfaces to the desired checkpointing system is
the {\em only} checkpoint plugin that must be loaded in each process
-- all others are unnecessary.

%%%%%

\paragraph{Dynamic frameworks.} 

Most frameworks allow their components to be loaded at run-time via
DSOs.  This is the most flexible method of finding and loading
components; it allows features such as explicitly {\em not} loading
certain components, loading 3rd party components that were not
included in the main line Open MPI distribution, etc.

%%%%%

\paragraph{Static frameworks.} 

Some one-of-many frameworks have additional constraints that force
their one-and-only-one component to be selected at compile time (vs.\
run time).  Statically linking one-of-many components allows direct
invocation of its member functions, which may be important in highly
performance-sensitive functionality (e.g., the {\tt memcpy} framework).

Additionally, some frameworks provide functionality that may need to
be utilized before Open MPI is fully initialized.  
%
For example, the use of some network stacks require complicated memory
registration models, which, in turn, require replacing the C library's
default memory management implementation.  
%
Since memory management is intrinsic to an entire process, replacing
the default scheme can only be done pre-{\tt main}.

Therefore, such components must be statically linked into Open MPI
processes so that they can be available for pre-{\tt main} hooks, long
before MPI has even been initialized.

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Plugin Components}

Open MPI plugins are divided into parts: a {\em component} struct
and a {\em module} struct.
%
The division is somewhat analogous to C++ classes and objects.
%
There is only one component per process; it describes the overall
plugin with some fields that are common to all components (regardless
of framework).
%
If the component elects to run, it is used to generate one or more
modules, which typically perform the bulk of the functionality
required by the framework.

%%%%%

\paragraph{Component struct.} 

Regardless of framework, each component contains a well-known,
statically-allocated and initialized component struct.  
%
The struct must named according to the template {\tt
  mca\_\-<framework>\_\-<component>\_\-component}.  For example, in
the Byte Transfer Layer (BTL) framework -- the MPI point-to-point
messaging framework -- the TCP network driver component's struct is
named {\tt mca\_\-btl\_\-tcp\_\-component}.

Having templated component symbols both guarantees that there will be
no name collisions between components, and allows the MCA core to find
any arbitrary component struct via {\tt dlsym(2)} (or the appropriate
equivalent for each different operating system).

The component struct contains some logistical information, such as the
component's formal name, version, framework version adherence, etc.
%
This data is used for debugging purposes, inventory listing, and
run-time compliance and compatibility checking.

The component also contains some function pointers:

\begin{aosaitemize}
\item {\em Parameter registration.} This function is typically the
  first function called on a component.  It allows the component to
  register any relevant run-time, user-settable parameters.  Run-time
  parameters are discussed further below.

\item {\em Open.} The {\em open} call is the initial query function
  invoked on a component.  It allows a component to initialize itself,
  look around the system where it is running, and determine whether it
  wants to be run.  If a component can always be run, it can not
  provide an open function.

\item {\em Query.} This call is a generalized ``do you want to run?''
  function.  Not all frameworks utilize this call -- some need more
  specialized query functions.

\item {\em Close.} When a framework decides that a component is no
  longer needed, it calls the {\em close} function to allow the
  component to release any resources that it has allocated.  The close
  function is invoked on all remaining components when processes are
  shutting down.  However, {\em close} can also be invoked on
  components that are rejected at run-time so that they can be closed
  and ignored for the duration of the process.
\end{aosaitemize}

The component structure can also be extended on a per-framework and/or
per-component basis.  Frameworks typically create a new component
struct with the generalized component struct as the first member.
%
This nesting allows frameworks to add their own attributes and
function pointers.  
%
For example, frameworks that need a more specialized query function
than the generic {\em query} function provided on the basic component
can add a function pointer in its framework-specific component struct.

The MPI {\tt coll} framework, which is used for collective MPI
operations, uses this technique.  Its framework-specific struct looks
like this:

\begin{aosabox}{Base component struct}
\begin{verbatim}
struct mca_coll_base_component_2_0_0_t {
    /* Base component struct */
    mca_base_component_t collm_version;
    /* Base component data block */
    mca_base_component_data_t collm_data;

    /* coll-framework specific query functions */
    mca_coll_base_component_init_query_fn_t collm_init_query;
    mca_coll_base_component_comm_query_2_0_0_fn_t collm_comm_query;
};
\end{verbatim}
\end{aosabox}

Similarly, any given plugin can extend the framework-specific
component struct with its own members.  
%
The {\tt tuned} component in the {\tt coll} framework does this; it
caches some algorithm rules and other meta data on its component
struct.  

\begin{aosabox}{coll-framework specific struct}
\begin{verbatim}
struct mca_coll_tuned_component_t {
    /* coll framework-specific component struct */ 
    mca_coll_base_component_2_0_0_t super;
    
    /* Priority of this component (set by the user at run-time) */
    int tuned_priority;
    
    /* Cached decision table */
    ompi_coll_alg_rule_t *all_base_rules;
};
\end{verbatim}
\end{aosabox}

This struct-nesting technique is effectively a simple emulation of C++
single inheritance: a pointer to an instance of a {\tt struct
  mca\_\-coll\_\-tuned\_\-component\_\-t} can be cast to any of the
three types can be used by an abstraction layer than does not
understand the ``derived'' types.

In general, casting is frowned upon because it can lead to incredibly
subtle, difficult-to-find bugs.  Casting is therefore usually {\em
  only} used for this C++-emulation technique.

%%%%%

\paragraph{Module struct.}

Module structs are individually defined by each framework; there is
not necessarily much commonality between them.
%
Depending on the framework, components generate one or more module
struct instances (a.k.a., ``modules'') when they indicate that they
want to be used.
%
For example, in the BTL framework, one module usually corresponds to a
single network device.  
%
If an MPI process is running on a Linux server with four ``up''
Ethernet NICs, the TCP BTL component will generate four TCP BTL
modules; each module will be assigned to a different Linux Ethernet
devices.
%
Each module will then be responsible for all sending and receiving
activity to and from that Ethernet device.

Composing BTL modules this way allows the upper layer MPI progression
engine to both treat all network devices equally, and to perform
user-level channel bonding.  
%
In the above 4-NIC example, a sender will strip large messages across
all TCP BTL modules that represent Ethernet devices that can be used
to reach the peer to which the message is being sent (reachability is
determined by TCP networks and netmasks, and some well-defined
heuristics).

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Run-Time Parameters}

Developers commonly make decisions when writing code, such as:

\begin{aosaitemize}
\item Should I use algorithm $A$ or algorithm $B$?
\item How large of a buffer should I preallocate?
\item How long should the timeout be?
\item At what message size should I change network protocols?
\item ...and so on
\end{aosaitemize}

Most users assume that the developers will make reasonable decisions
for most cases.  
%
However, the HPC community is full of scientist and engineer power
users who aggressively want to tweak their hardware and software
stacks to eek out every possible last compute cycle.
%
Although these users do not typically want to tinker with the code of
the innards of their MPI implementation, they {\em do} want to tinker
by selecting different algorithms, choosing different resource
consumption patterns, or forcing specific network protocols in
different circumstances.

Therefore, when designing Open MPI, we included a flexible run-time
tuning mechanism called the MCA parameter system.
%
Developers can register string or integer MCA parameters from anywhere
in the Open MPI code base, along with an associated default value and
help string defining what the parameter is and how it is used.
%
The general rule of thumb is that instead of using constants in code,
use an MCA parameter instead, and allow a power user to tweak the
behavior if they want to.

There are a number of parameters in each of the the abstraction
layers, but parameters are much more common in components.
%
For example, the TCL BTL plugin has a parameter that specifies whether
only TCPv4, only TCPv6, or both should be used.
%
Another TCP BTL parameter specifies which Ethernet devices to use.

Users can discover what parameters are available via a user-level
command line tool ({\tt ompi\_\-info}).
%
Parameter values can be set in multiple ways: on the command line, via
environment variables, via the Windows registry, or in system- or
user-level INI-style files.

This run-time parameter system has proved to be invaluable; although
Open MPI developers try to choose reasonable defaults for a wide
variety of situations, every HPC environment is different.  There will
inevitably environments where Open MPI's defaults will be unsuitable
-- and possibly even detrimental to performance.  
%
Having the MCA parameter system allows users to be proactive and tweak
Open MPI's behavior for their environment.  Not only does this
alleviate many upstream requests for changes and/or bug reports, it
allows users to experiment with the parameter space to find the best
configuration for their specific system.

\end{aosasect2}

\end{aosasect1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{aosasect1}{Lessons Learned}

With such a varied group of developers, it is inevitable that we would
each learn {\em something}, and that as a group, we would learn many
things.

%--------------------------------------------------------------------------

\begin{aosasect2}{Performance}

Message passing performance and resource utilization are the king
and queen of HPC.
%
Open MPI was specifically designed in such a way that it could operate
that the very bleeding edge of high performance: incredibly low
latencies for sending short messages, extremely high short message
injection rates on supported networks, fast ramp-ups to maximum
bandwidth for large messages, etc.  
%
Abstraction is good (for many reasons), but it must be designed with
care so that it does not get in the way of performance.  Or, put
differently: carefully choose abstractions that lend themselves to
shallow, performant call stacks (vs.\ deep, feature-rich API call
stacks).

That being said, we also had to accept that in some cases, abstraction
must be thrown out the window.  Case in point: Open MPI has hand-coded
assembly for some of its most performance-critical operations, such as
shared memory locking and atomic operations.

It is worth noting that Figures~\ref{fig.openmpi.layers}
and~\ref{fig.openmpi.mca} show two different {\em architectural} views
of Open MPI.
%
They do not represent the run-time call stacks or calling invocation
layering for of the high performance code sections.

%%%%%

\paragraph{Lesson learned:} 

It is acceptable -- albeit undesirable --and unfortunately sometimes
necessary to have gross, complex code in the name of performance
(e.g., the aforementioned assembly code).
%
However, it is {\em always} preferable to spend time trying to figure
out how to have good abstractions to discretize and hide complexity
whenever possible.  A few weeks of design can literally save hundreds
or thousands of man-hours of maintenance on tangled, subtle, spaghetti
code.

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Standing on the Shoulders of Giants}

We actively tried to avoid re-inventing code in Open MPI that someone
else has already written (when such code is compatible with Open
MPI's BSD licensing).
%
Specifically, we have no compunction against either directly re-using
someone or interfacing to else's code.

There is no place for the ``not invented here'' religion when trying
to solve highly complex engineering problems; it only makes good
logistical sense to re-use external code whenever possible.
%
Such re-use frees developers to focus on the problems unique to Open
MPI; there is no sense re-solving a problem that someone else has
solved already.

A good example of this kind of code re-use is the Libtool Libltdl
package.  Libltdl is a small library that provides a portable API for
opening DSOs and finding symbols in them.  Libltdl is supported on a
wide variety of operating systems and environments, including
Microsoft Windows.

Open MPI {\em could} have provided this functionality itself -- but
why?
%
Libltdl is a fine piece of software, is actively maintained, is
compatible with Open MPI's license, and provides exactly the
functionality that was needed.
%
Given these points, there is no realistic gain for Open MPI developers
to re-write this functionality.

%%%%%

\paragraph{Lesson learned:} 

When a suitable solution exists elsewhere, do not hesitate to integrate
it and stop wasting time trying to re-invent it.

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Optimize for the Common Case}

Another guiding architectural principle has been to optimize for the
common case.  
% 
For example, emphasis is placed on splitting many operations into two
phases: setup and repeated action.  The assumption is that setup may
be expensive (meaning: slow).  So do it {\em once} and get it over
with
%
Optimize for the much more common case: repeated operation.

For example, {\tt malloc()} can be slow, especially if pages need to
be allocated from the operating system.  So instead of allocating just
enough bytes for a single incoming network message, allocate enough
space for a {\em bunch} of incoming messages, divide the result up
into individual message buffers, and setup a freelist to maintain
them.  In this way, the {\em first} request for a message buffer may
be slow.  But {\em successive} requests will be much faster because
they will just be de-queues from a freelist.

%%%%%

\paragraph{Lesson learned:} Split common operations into (at least)
two phases: setup and repeated action.  Not only with the code perform
better, it may be easier to maintain over time because the distinct
actions are separated.

\end{aosasect2}

%--------------------------------------------------------------------------

\begin{aosasect2}{Miscellaneous}

There are too many more lessons learned to describe in detail here,
so the following is an abbreviated list that will hopefully be useful:

\begin{aosaitemize}
\item We were fortunate to draw upon 15+ years of HPC research and
  make designs that have (mostly) successfully carried us for more
  than sever years.  When embarking on a new software project, {\em
    look to the past}.  Be sure to understand what has already been
  done, why it was done, and what its strengths and weaknesses were.

\item The concept of components -- allowing multiple different
  implementations of the same functionality -- has saved us many
  times, both technically and politically.  Plugins are good.

\item When adding another plugin was not sufficient (for whatever
  reason), we added a framework instead.  This usually created enough
  flexibility to solve whatever problem had arisen.

\item Space vs.\ time can be an appropriate trade-off.  For example,
  allocating space for $N$ incoming messages can be a net performance
  win because, for example, individual {\tt malloc} overhead can be
  amortized across receiving multiple messages.

\item Accelerators (such as generalized GPUs) are challenging some of
  our initial designs; we didn't account for their cross-cutting
  behavior in the initial Open MPI designs.  They are forcing us to
  re-think some of our base assumptions.

\item Differences are good.  Developer disagreements are good.
  Embrace challenges to the status quo; do not get complacent.  A
  plucky grad student saying ``hey, check this out...'' can lead to
  the basis of a whole new feature or a major evolution of the
  product.

\item Although outside the scope of this book, people and community
  matter.  A lot.

\end{aosaitemize}

\end{aosasect2}

\end{aosasect1}

\end{aosachapter}

% LocalWords:  rsh SLURM LSF NIC Plugin DSOs memcpy composability plugin pre de
% LocalWords:  typedefs structs struct mca btl tcp dlsym NICs TCL TCPv ompi INI
% LocalWords:  discretize malloc freelist
