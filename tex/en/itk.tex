\begin{aosachapter}{ITK}{s:itk}{Luis Ib\'{a}\~{n}ez and Brad King}

% Your chapter goes here---please look at /volume1/tex/en/wesnoth.tex for
% formatting ideas.

\begin{aosasect1}{What Is ITK?}
The Insight Toolkit ITK\footnote{\url{http://www.itk.org}} is a library for
image analysis that was developed by the initiative, and mainly with the
funding, of the US National Library of
Medicine\footnote{\url{http://www.nlm.nih.gov}}. ITK can be tought of as a
usable encyclopedia of image analysis algorithms, in particular for image
filtering, image segmentation and image registration. The library was developed
by a consortium involving universities, commercial companies, and many
individual contributors from around the world.  Development of ITK started in
1999, and recently after its 10th anniversary the library underwent a
refactoring process intended to remove crusty code and to reshape it for the
next decade.
\end{aosasect1}

\begin{aosasect1}{Architectural Features}

Software toolkits have a very synergistic relationship with their communities.
They shape one another in a continuous iterative cycle.  The software gets to
be modified until it satisfies the needs of the community, while the community
behaviors themselves are adapted based on what the software empowers or
restricts them to do. In order to better understand the nature of ITK's
architecture, it is therefore very useful to get a sense of what kind of
problems the ITK community is usually addressing and how they tend to go about
them.

\begin{aosasect2}{The Nature of the Beast}

\begin{center}
\begin{quotation}
\emph{
``If you don't understand the nature of the beast,\\
it would be of little use to know the mechanics of their anatomy''.\\
}
\hfill Dee Hock - \emph{One From Many}
\end{quotation}
\end{center}

In a typical image analysis problem, a researcher or an engineer will take an
input image, will improve some characteristics of the image by let's say
reducing noise or increasing contrast, and then will proceed to identify some
features in the image, such as corners and strong edges. This type of
processing is naturally well suited for a data pipeline architecture, as
shown in Figure~\ref{fig.itk.pipeline}.

To illustrate this point, Figure~\ref{fig.itk.brim} below, shows an image of a
brain from a magnetic resonance image (MRI), and the result of processing it
with a median filter to reduce its level of noise, as well as the outcome of an
edge detection filter used to identify the borders of anatomical structures.

\aosafigure{../images/itk/ExampleImageProcessingPipeline.pdf}{Image Processing Pipeline}{fig.itk.pipeline}


%
% NOTE to the EDITOR: We meant for these images to be rather small, and to be
% arranged in a single row, from left to right, but didn't quite found how
% to resize them with the parameters of the aosafigure command.
%
% \aosafigure{../images/itk/BrainProtonDensitySlice.png}{MRI Brain Image}{fig.itk.brim}
% \aosafigure{../images/itk/BrainProtonDensitySliceMedian.png}{Median Filter}{fig.itk.brimmedian}
% \aosafigure{../images/itk/BrainProtonDensitySliceCanny.png}{Edge Detection Filter}{fig.itk.brimcanny}
%
\begin{figure}[h!]
\centering
\includegraphics[width=0.3\textwidth]{../images/itk/BrainProtonDensitySlice.png}
\includegraphics[width=0.3\textwidth]{../images/itk/BrainProtonDensitySliceMedian.png}
\includegraphics[width=0.3\textwidth]{../images/itk/BrainProtonDensitySliceCanny.png}
\caption{From left to right: MRI Brain Image, Median Filter, Edge Detection Filter}
\label{fig.itk.brim}
\end{figure}

For each one of these tasks, the image analysis community has developed a
variety of algorithms, and continue developing new ones. \emph{``Why do they
continue doing this?''} you may ask, and the answer is that image processing is
a combination of science, engineering, art, and ``cooking'' skills. Claiming
that there is an algorithmic combination that is the ``right'' answer to an
image processing task, is as mislead as claiming that there is such a thing as
the ``right'' type of chocolate dessert for a dinner. Instead of pursuing
perfection, the community strives for producing a rich enough set of tools that
ensures that there will be no shortage of options to try when it comes the
time to face a given image processing challenge. This state of affairs, of
course, comes at a price. The cost is that the image analyzer is confronted with
the difficulty of choosing among tens of different tools that could be used in
different combinations to achieve similar results.

The image analysis community is also closely integrated with the research
community. It is common to find specific research groups that become attached
to the algorithmic families they have developed. This custom of ``branding'',
and up to some level ``marketing'', leads to a situation where the best that the
software toolkit can do is to offer the community a very complete set of
algorithmic implementations that they can try, and then mix and match to create
a recipe that satisfies their needs. These are some of the reasons why ITK was
designed and implemented as a large collection of somewhat independent tools,
many of which can be used to solve similar problems. In this context, a certain
level of ``redundancy'' is not seen as a problem but as a valuable feature. The
toolkit was also conceived as a resource that will grow and renew itself
continously, as new algorithms and better implementations become available
superseeding exiting ones, as well as new tools are developed in response to
the emerging needs of new medical imaging technologies.

Armed with this quick overview of the nature of activities that make the daily
routine of the image analyzers who compose the ITK community, we can now dive
into the main features of the architecture:

\begin{aosaitemize}
\item Modularity
\item Data Pipeline
\item Factories
\item IO Factories
\item Streaming
\item Maintainability
\item Reusability
\end{aosaitemize}

\end{aosasect2}

\begin{aosasect2}{Modularity}
Modularity is one of the main characteristics of ITK. This is a requirement
that emerges from the way people in the image analysis community work when
solving their problems. Most image analysis problems require to put one or more
input images through a combination of processing filters that enhance or
extract particular pieces of information from the images. There is therefore,
no single large processing object, but a myriad of small ones. This structural
nature of the image processing problem maps to implement the software as a
large collection of image processing filters that can be combined in many
different ways.

It is also the case that certain types of processing filters are clustered into
families, inside which some of their implementation features can be factorized.
This leads to natural grouping of the image filters into modules and groups of
modules.

Modularity, therefore occurs at two natural levels in ITK:

\begin{aosaitemize}
\item Filter Level
\item Filter Family Level
\end{aosaitemize}

At the image filter level, ITK has a collection of about 500 filters. Given
that ITK is implemented in C++, this is a natural level at which every one of
those filters is implemented by a \textbf{C++ Class} following and Object
Oriented design.  At the filter family level, ITK groups filters together
according to the nature of the processing that they perform. For example, all
filters that are related to Fourier Transforms will be put together into a
\textbf{Module}.  At the C++ level, Modules maps to directories in the source
tree, and to libraries once the software is compiled to its binary form. Each
module contains:

\begin{aosaenumerate}

\item The source code of the image filters that belong to that family.

\item A set of configuration files that describe how to build the module and
list dependencies between this module and other modules.

\item The set of unit tests corresponding to each one of the filters.

\end{aosaenumerate}

This hierarchichal structure is illustrated in
Figure~\ref{fig.itk.modulehierarchy}.

\aosafigure{../images/itk/IllustrationOfModularStructure.pdf}{Hierarchical Structure of Groups, Modules and Classes}{fig.itk.modulehierarchy}


ITK currently has 98 modules, that are in turn aggregated into 16 major groups.
The modules have a variety of different sizes. This size distribution, in
bytes, is presented in Figure~\ref{fig.itk.modulesize}.

\aosafigure{../images/itk/moduleSizePlot.pdf}{Distribution of Module Size in Bytes}{fig.itk.modulesize}
\aosafigure{../images/itk/moduleSizePlotNoThirdParty.pdf}{Distribution of Module Size in Bytes without Third Party Libraries}{fig.itk.modulesizenothirdparty}

The modularization in ITK applies as well to a set of third party libraries
that are not directly part of the tookit, but that the toolkit depend upon, and
that are distributed along with the rest of the code for the convinience of
users. Particular examples of these third party libraries are the image file
format libraries: HDF5, PNG, TIFF, JPEG and openjpeg among others. When these
libraries are excluded from the analysis of module size, the distribution
become the one shown in Figure~\ref{fig.itk.modulesizenothirdparty}

The modular architecture of ITK enables and facilitates:

\begin{aosaitemize}
\item Reduction of cross-dependencies
\item Adoption of code contributed by the community
\item Evaluation of quality metrics per module (for example: code coverage)
\item Building selected subsets of the toolkit
\item Packaging selected subsets of the toolkit for redistribution
\item Continued growth by progressive addition of new modules
\end{aosaitemize}
\end{aosasect2}

Again, we can recognize in these items the fact that the structure of the
toolkit reflects the organization of the community and in some cases the
processes that have been adopted for the continous growth and quality control
of the software.

\begin{aosasect2}{Data Pipeline}
The staged nature of most image analysis tasks led naturally to the selection
of a Data Pipeline architecture as the backbone infrastructure for data
processing. The Data Pipeline enables:

\begin{aosaitemize}

\item \textbf{Filter Concatenation:} A set of image filters can be concatenated
one after another, composing a processing chain in which a sequence of
operations are applied to the input images.

\item \textbf{Parameter Exploration:} Once a processing chain is put together,
it is easy to change the parameters of any filter in the chain, and to explore
the effects that such change will have on the final output image.

\item \textbf{Memory Streaming:} Large images can be managed by processing only
sub-blocks of the image extent at a time. In this way, it becomes possible to
process large images that otherwise would have not fitted in main memory.

\end{aosaitemize}

\aosafigure{../images/itk/DataPipelineIllustrationInRegionGrowingApplication.png}{GUI Illustration of a Data Pipeline for performing region growing segmentation}{fig.itk.regiongrowinggui}

Figures~\ref{fig.itk.pipeline} and~\ref{fig.itk.brim} have already presented a
simplified representation of a data pipeline from the image processing point of
view. A more detailed case can be seen in
Figure~\ref{fig.itk.regiongrowinggui}, for a simple application that exposes
the Data Pipeline connections in its GUI, along with the set of numeric
parameters that can be adjusted for each one of the intermediate filters. Every
time that one of the numeric parameters is modified, the data pipeline marks
its output as ``dirty'' and knows that this particular filter, and all the
downstream ones that use its output, should be executed again. This feature of
the pipeline facilitates the exploration of the parameter space while
maintainting to a minimum the amount of processing power that have to be
invested at every instance of an experiment.

Two main types of objects were designed to hold the basic structure of the
pipeline.  They are the \code{DataObject} and the \code{ProcessObject}. The
\code{DataObject} is the abstraction of classes that carry data. For example,
images and geometrical meshes. The \code{ProcessObject} provides an abstraction
for the image filters and mesh filters that process such data.
\code{ProcessObjects} take \code{DataObjects} and perform some type of
algorithmic transformation on them, such as the ones illustrated
in Figure~\ref{fig.itk.brim}.

\aosafigure{../images/itk/ProcessObjectDataObject.pdf}{Relationship between ProcessObjects and DataObjects}{fig.itk.processobjectdataobject}

DataObjects are generated by ProcessObjects. This chain typically starts by
reading a DataObject from disk, for example by using a FileImageReader which is
a type of ProcessObject. The ProcessObject that created a given DataObject is
the only one that should modify such DataObject. This output DataObject is
typically passed as input to another ProcessObject downstream in the pipeline.
This sequence is illustrated in Figure~\ref{fig.itk.processobjectdataobject}.
The same DataObject may be passed as input to multiple ProcessObjects, as it is
shown in the Figure for the DataObject constructed by the FileReader at the
beginning of the pipeline. It is also common for some filters to require two
DataObjects as input, as it is the case of the ``Subtract Filter'' illustrated
in the right side of the same figure.

The initial design and implementation of the Data Pipeline in ITK was derived
from the one in the Visualization Toolkit (VTK)\footnote{See \emph{Architecture
of Open Source Applications}, Volume 1}, that was a mature project at the time
that ITK development was in its initial stages.

Figure~\ref{fig.itk.processobjectdataobjecthierarchy} shows the Object Oriented
hierarchy of the pipeline Objects in ITK. In particular the relationship
between the basic \code{Object}, \code{ProcessObject}, \code{DataObject}, and
some of the classes in the filter family and the data family. In this
abstraction, any object that is expected to be passed as input to a filter, or
be produced as output by a filter, must derive from the \code{DataObject}. All
filters that produce and consume data are expected to derive from the
\code{ProcessObject}. The data negotiations required to move data through the
pipeline are implemented part in the \code{ProcessObject} and part in the
\code{DataObject}.

\aosafigure{../images/itk/ProcessObjectDataObjectHierarchy.pdf}{Hierarchy of ProcessObjects and DataObjects}{fig.itk.processobjectdataobjecthierarchy}

\end{aosasect2}

\begin{aosasect2}{Factories}

One of the fundamental design requirements of ITK is to provide support for
multiple platforms. This requirement emerges from the desire to maximize the
impact of the toolkit by making it usable to a broad community regardless of
their platform of choice. ITK adopted the design pattern of ``Factories'' to
address the challenge of supporting fundamental differences among the many
hardware and software platforms, without sacrificing the good fitness of a
solution to each one of the individual platforms.

The Factory Pattern in ITK uses class names as keys to a registry of class
constructors. The registration of Factories happens at run time, and can be
done by simply placing dynamic libraries in specific directories that ITK
applications search at start up time. This last feature provides a natural
mechanism for implementing a Plugin Architecture in a clean and transparent
way. The outcome is to facilitate the development of extensible image analysis
applications, satisfying the requirements of providing an ever-growing set of
image analysis capabilites.

\end{aosasect2}

\begin{aosasect2}{IO Factories}

The image analysis community has developed a very large set of file formats to
store image data. Many of these file formats are designed and implemented with
specific uses in mind, and therefore are fine-tunned to specific types of
images. As a consequence, on a regular basis, new image file formats are
conceived and promoted across the community. Aware of this situation, the ITK
developement team designed an IO Architecture suitable for unlimited
extensibility, in which it is easy to add support for more and more file
formats on a regular basis.

\aosafigure{../images/itk/ImageIOFactoriesDesignPattern.pdf}{IO Factories Dependencies}{fig.itk.io.factoriesregistry}

These IO extensible architecture is built upon the Factories mechanism
described in the previous section. The main difference is that in the IO case,
the IO Factories are registered in a specialized registry that is managed by
the \code{ImageIOFactory} base class, shown on the upper left corner of
Figure~\ref{fig.itk.io.factoriesregistry}. The actual functionality of reading
and writing data from image file formats is implemented in a family of
\code{ImageIO} classes, shown on the right side of
Figure~\ref{fig.itk.io.factoriesregistry}. These service classes are intended
to be instantiated on deman when the user request to read or write an image.
The services classes are not exposed to the application code. Instead,
application are expected to interact with the facade classes:

\begin{aosaitemize}
\item \code{ImageFileReader}
\item \code{ImageFileWriter}
\end{aosaitemize}

These are the two classes with which the application will invoke code such as:

\begin{aosaitemize}
\item \code{reader->SetFileName(``image1.png'');}
\item \code{reader->Update();}
\end{aosaitemize}

or

\begin{aosaitemize}
\item \code{writer->SetFileName(``image2.jpg'');}
\item \code{writer->Update();}
\end{aosaitemize}

The self-contained nature of every IO Factory and ImageIO service classes is
also reflected in the modularization. Typically, an ImageIO class depends on a
specialized library that is dedicated to managing a specific file format. That
is the case for PNG, JPEG, TIFF and DICOM, for example. In those cases, the
third party library is managed as a self-contained module, and the specialized
ImageIO code that interfaces ITK to that third party library is also put in a
Module by itself. In this way, specific applications may disable many
fileformats that are not relevant for their domain, and can focus on offering
only those fileformats that are useful for the anticipated scenarios of this
application.

Just as with standard Factories, the IO Factories can also be loaded at
run-time from dynamic libraries. This flexibility facilitates the use of
specialized and in-house developed file formats without requiring all such file
formats to be incorporated directly into the ITK toolkit. The loadable IO
factories has been one of the most successful features in the Architectural
design of ITK. It has made possible to easily manage a challenging situation
without placing a burden in the code nor obscuring its implementation. More
recently, the same IO architecture has been adapted to manage the process of
reading and writing files containing spatial transformations represented by the
\code{Transform} class family.

\end{aosasect2}

\begin{aosasect2}{Streaming}
ITK was conceived initially as a set of tools for processing the images
acquired by the Visible Human
Project\footnote{\url{http://www.nlm.nih.gov/research/visible/visible_human.html}}.
At the time, it was clear that such a large dataset would not fit in the RAM of
computers that were typically available to the medical imaging research
community. It is still the case that the dataset will not fit in the typical
desktop computers that we use today. Therefore, one of the requirements for
developing the Insight Toolkit was to enable the streaming of image data
through the Data Pipeline. More specifically, to be able to process large
images by pushing sub-blocks of the image throught the data pipeline, and then
assembling the resulting blocks on the output side of the pipeline.

Streaming, unfortunately, can not be apply to all types of algorithms. Specific
cases that are not suitable for streaming are:

\begin{itemize}
\item Iterative algorithms that, to compute a pixel value at every iteration,
require to use as input the pixel values of its neighbors. This is the case of
most PDE-solving-based algorithms, such as anisotropic diffusion, demons
deformable registration, and dense level sets, for example.
\item Algorightms that requires the full set of input pixel values in order to
compute the value of one of the output pixels. Fourier transform and Infinite
Impulse Response (IIR) filters, such as the Recursive Gaussian filter, are
examples of this class.
\item Region propagation or front propagation algorithms in which the
modification of pixels also happens in an iterative way but for which the
location of the regions or fronts can not be systematically partitioned in
blocks in a predictable way. Region growing segmentation, sparse level sets,
some implementations of mathematical morphology operations and some forms of
watersheds are typical examples here.
\item Image registration algorithms, given that they require to have access to
the full input image data for computing metric values at every iteration of
their optimization cycles.
\end{itemize}

Fortunately, on the other hand, the data pipeline structure of ITK enables
support for streaming in a variety of transformation filters by taking
advantage of the fact that all filters create their own output, and therefore
they do not overwrite the memory of the input image. This comes at the price of
memory consumption, since the pipeline has to allocate both the input and
output images in memory simultaneously. Filters such as flipping, axes
permutation, and geometric resampling fall in this category. In these cases,
the data pipeline manages the matching of input regions to output regions by
requiring every filter to provide a method called
\code{GenerateInputRequestedRegion()} that takes as argument a rectangular
output region. This method computes the rectangular input region that will be
needed by this filter to compute that specific rectangular output region. This
continuous negotiation in the data pipeline makes possible to find for every
output block the corresponding section of input image that is required for
computation.

To be more precise here, we must say therefore that ITK supports
Streaming\ldots but only in the algorithms that are \emph{``streamable''} in
nature. That said, in the spirit of being progressive regarding the remaining
algorithms, we should qualify this statement by not claiming that ``it is
impossible to stream such algorithms'', but rather say that ``our typical
approach to streaming is not suitable for these algorithms'' at this point, and
that hopefully new techniques will be devised by the community in the future to
addresss these cases.
\end{aosasect2}

\end{aosasect1}

\begin{aosasect1}{Lessons Learned}

\begin{aosasect2}{Reusability}
\end{aosasect2}

\begin{aosasect2}{Maintainability}
The architecture satisfies the constraints that minimize maintenace cost.
\begin{aosaitemize}
\item Modularity (at the class level)
\item Many small files
\item Code reuse
\item Repeated patterns
\end{aosaitemize}
\end{aosasect2}
As the developers got involved in regular maintenance activities, they
got exposed to the ``common failures'' of certain details. The things
that raised common questions in the mailing lists, the details that
new developers tend to miss and that led them to introduce buggy code.
After dealing with such issues, developers learned to write code that
is ``good for maintenance''. Some of this traits apply to both coding
style and the actual organization of the code.

\begin{aosasect2}{The Invisible Hand}
The software should look like writted by a single person. The best
developers are the ones that write code that can be taken over by
anybody else, should they be taken down by the ``Provervial Bus'' when
crossing a street. We have grown to recognize that any trace of
``personal touch'' is an indication of a defect introduced in the
software.
\end{aosasect2}

\begin{aosasect2}{Refactoring}
ITK Started in the year 2000 and grew continuously until the year
2011. The development team had the truly unique opportunity to embark
in a refactoring effort under the funding of the National Library of
Medicine. This is not a minor feat. Once you have been working on a
piece of software for over a decade, and you are offered the
opportunity to clean it up: What would you change ?.
\end{aosasect2}

\begin{aosasect2}{Reproducibility}

One of the early lessons learned in ITK was that the many papers
published in the field were not as easy to implement as we were led to
believe. The computational field tend to over-celebrate algorithms and
to dismiss the practical work of writing software as ``just an
implementation detail''. That dismissive attitude is quite damaging to
the field, since diminshes the importance of the first-hand experience
with the code and its proper use. The outcome is that most published papers
are not reproducible, and that when researchers and students attempt to use
such techniques, they end up spending a lot of time in the process and deliver
variations of the original work. It is actually quite difficult in practice
to verify if an implementation matches what was described in a paper.

ITK disrupted, for the good, that environment and restored a culture of DYI (Do
It Yourself), in a field that has grown accoustomed to theoretical reasoning,
and that had learned to dismiss experimental work. The new culture brought by
ITK is a practical and pragmatic one in which the virtues of the software are
judged by its practical results and not by the appearance of complexity that is
celebrated in scientific publications. Helas, it turns out that in practice,
the most effective processing methods are those that would appear to be too
simple to be accepted for a scientific paper.

The culture of Reproducibilty is a continuation of the philosophy of Test
Driven Developement, and systematically results in better software. Higher
clarity, readability, robustness and focus.

The Insight Journal...
\end{aosasect2}

\end{aosasect1}

\end{aosachapter}
