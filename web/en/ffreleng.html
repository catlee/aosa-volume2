<!-- vim: set ts=2 sw=2 tw=70: -->
<!-- vim: set autoindent -->
<html>
  <head>
    <style type="text/css">
      div.inline { float:left; }
      code.class-name {
        margin: 1em;
        padding: 1em;
        display: inline-block;
        background: #ccc;
        border: 1px dotted #000;
        overflow: auto;
      }
      .figure {
        font-style: italic;
      }
    </style>
    <meta name="provenance" content="$Id$" />
    <link rel="stylesheet" href="aosa.css" type="text/css" />
    <title>The Architecture of Open Source Applications, Volume 2: Firefox Release Engineering</title>
  </head>
  <body>
    <div class="header">
      <table>
    <tr>
      <td>
        <a href="index.html"><img src="../images/titlebar.jpg" alt="The Architecture of Open Source Applications, Volume 2" /></a>
      </td>
      <td>
        <strong><em>The Architecture of Open Source Applications, Volume 2</em></strong>
        <br/>
        <strong>Amy Brown and Greg Wilson (eds.)</strong>
      </td>
    </tr>
      </table>
      <h1 class="chaptitle">Firefox Release Engineering</h1>
      <h1 class="chapterauthor">
        <a href="intro.html#atlee-chris">Chris AtLee</a>, 
        <a href="intro.html#blakk-lukas">Lukas Blakk</a>,
        <a href="intro.html#oduinn-john">John O'Duinn</a>, 
        <a href="intro.html#zambrano-gasparnian-armen">Armen Zambrano Gasparnian</a>
      </h1>
    </div>
<!-- FORMATTING -->
<!-- Sections go into divs
<div class="sect"></div>
-->
<!-- Headers are level 2 with the following format:
<h2>{Chapter#}.{Section#} {Section Title}</h2>
-->
<!-- Then mostly normal markup for paragraphs
<p>Lorem Ipsum...</p>
-->
<!-- Example of how to incorporate a diagram
<div class="figure" id="fig.ffreleng.arch">
  <img src="../images/ffreleng/diagram.png" alt="[Image Title]" />
  <p>Figure&nbsp;{Chapter#}.{Section#}: {Image Title}</p>
</div>
-->
    <!-- Introduction does not need a section div -->
    <p>
    The Mozilla Release Engineering team has made a lot of advances
    recently in how our release automation works. What we have so far
		is not perfect, but it's getting much closer to our goal of being
		able to push a button and walk away, with minimal human
		interventions, eliminating many of the headaches and do-overs we
		experienced with our older part manual, part automated release
		processes. In this chapter, we will explore and explain the
		infrastructure decisions as well as scripts that comprise the
		complete, and current, Firefox rapid release system.
    </p>
    <p>
		You'll follow the system from the perspective of a release-worthy
		Mercurial changeset as it is turned into a release candidate - and
		then a public release - available to over 400 million daily users
		worldwide.  We'll start with builds &amp; code signing, then
		customized partner and localization repacks, the QA process, and
		how we generate updates for every supported version, platform and
		localization. All of these steps must be completed before any
		release can be pushed out to Mozilla Community's network of
		mirrors which provide the downloads to our users.
    </p>
    <p>
    We'll look at some of the newer decisions that have been made to improve
    this process like our sanity-checking script that helps eliminate much
    of what used to be vulnerable to human error, our automated signing
    script, our integration of mobile releases into the desktop release
    stream, and the land of patcher/AUS where updates are created and served
    to our users across multiple versions of the software.
    </p>
    
    <!-- XXX TODO: talk about integration of mobile release -->
    <p>
		Follow a Mozilla Firefox release from the moment the Release
		Coordinator gives the official "Go" to when it's available for
		download (or update) to your computer or mobile device.
    </p>
    <div class="sect">
      <h2>6.0 "Look N ways before you start a release"</h2>
    <p>
    This chapter describes the mechanics of how we generate release
    builds for the Firefox product. Most of this chapter details the
		complexity once the release builds start, but there is also plenty
		of complex cross-group communications to deal with before Release
		Engineering even starts to generate release builds, so lets start
		there.
    </p>
    <p>
    When we started on the project to improve Mozilla's release
    process, we began with the premise that the more popular Firefox
    became, the more users we would have, the more attractive a target
		Firefox would become to "blackhat hackers" looking for security
		vulnerabilities to exploit. Also, the more popular Firefox became,
		the more users we would have to protect from a newly discovered
		security vulnerability, so the more important it would be to
		deliver a security fix as quickly as possible. We even have a
		term for this - a "chemspill" release. Instead of being surprised
		by the occasional need for a chemspill release in between our
		regularly scheduled releases, we decided to plan as if every
		release could be a chemspill release, and designed our release
		automation accordingly.
    </p>
    <p>
    This mindset has three important side-effects:
    <ol><li>We do a postmortem after <em>every</em> release, and look to
			see where things could be made smoother, easier, and faster next
			time. If at all possible, we find and fix any one thing
			immediately, no matter how small, before the next release. This
			constant polishing of our release automation means we're always
			looking for new ways to rely on less human involvement while
			also enhancing robustness and speeding up turnaround time. A lot
			of effort is spent making our tools and processes bulletproof so
			that "rare"
			events like network hiccups, disk space issues or typos made by
			real live humans are caught and handled as early as possible.
			Even though we're already fast enough for regular, non-chemspill
			releases, we continue to want to reduce the risk of any human
			error in a future release, especially in a chemspill
			release.</li> 
			<li>When we do have a chemspill release,
		the humans in Release Engineering are not stressed by it. We're
		used to the idea of going as fast as possible with calm precision,
		and we've built tools to do this as safely and robustly as we know
		how. Less stress means more calm and precise work, on a
		well-rehearsed process, which in turn helps chemspill releases go
		smoothly.</li>
		<li>We created a Mozilla-wide "go to build" process. When doing a
		a non-chemspill release, its possible to have everyone looking
		through the same bug triage queries, have everyone see clearly
		when the last fix was landed, and tested just fine, and have
		consensus on when to start builds. However, in a chemspill release
		- where minutes matter - keeping track of all the details of the
			issue, following up bug confirmations and fixes, gets very
			tricky very quickly.  To reduce complexity, and the risk of
			mistakes, Mozilla now has a full-time person, - someone
			explicitly not in Release Engineering - track the readiness of
			the code for a  "go to build"i decision.  Changing processes
			during a chemspill is risky, so in order to make sure everyone
			is familiar with the process when minutes matter, we use this
			same process for "chemspill" and "regular" releases.</li> </ol>
    </p>
  </div>
      <div class="figure" id="fig.ffreleng.timeline" class="inline">
        <img src="../images/ffreleng/timeline.png" alt="Complete Release Timeline" />
        <p>Figure&nbsp;6.0: Complete Release Timeline</p>
      </div>
    <div class="sect">
      <h2>6.1 "Go to build"</h2>
      <div class="figure" id="fig.ffreleng.go_to_build" class="inline">
        <img src="../images/ffreleng/go_to_build.png" alt="Getting from code to "Go to build" />
        <p>Figure&nbsp;6.1: Getting from code to "Go to build"</p>
      </div>

<h3>Who can send the "go to build"?</h3>

    <p>Before the start of the release, one person is designated
    to assume the responsibility for the entire release. It is
    worth noting that this person is not in Release Engineering. This
    person needs to be someone that everyone trusts to attend triage
		meetings, have background context on all the work being landed,
		referee bug severity disputes fairly, approve landing of late
		breaking changes, and make tough back-out decisions.  
		Additionally, for the actual release day, this person
		is on point for all communications with the different groups (developers, QA, Release
		Engineering, website developers, PR, marketing, ...). 
	

		Different companies use different titles for this type of role.
		Some titles are: Release Manager, Release Engineer (!), Program
		Manager, Project Manager, Product Manager, Product Czar, Release
		Driver. This chapter will use the term "Release Coordinator" as it
		most clearly defines the role in our process, but the important
		point is that the role, and the final authority of the role, is
		clearly understood by everyone before the release starts,
		regardless of their background, or previous work experiences
		elsewhere. In the heat of the moment of a release day, we all have
		to abide by, and trust, the final decision that this person makes.
    </p>
    <p>
    This Release Coordinator is also the only person outside of Release Engineering who is authorized
    to send "stop builds" emails if a show-stopper problem is discovered with
    the release. Any reports of "suspected show-stopper problems" are
    redirected to the Release Coordinator, who will evaluate, make the
    final go/no-go decision and communicate that decision to everyone
    in a timely manner.
    </p>

<h3>How to send the "go to build"?</h3>

    <p>
    Early experiments with sending "go to build" in IRC channels, or verbally
    over the phone, led to misunderstandings at some point, occasionally causing
		problems for the release in progress. Therefore, we now require
		that the "go to build" signal for every release is done by email
		to a mailing list that includes everyone across all
		groups involved in release processes. The subject of the email
		includes "go to build" and the explicit product name and version
		number, for example:<br />
    &nbsp;&nbsp;&nbsp;&nbsp;"go to build Firefox 6.0.1"
    </p>
    <p>
		Similarly, if a problem found in the release, then the Release
		Coordinator will send a new "all stop" email to the same mailing
		list, with a new subject line. We found that it was not ok to just
		hit reply on the most-recent email about the release - email
		threading of some email clients caused some people to not notice
		the "all stop" email if it was way down a long and unrelated thread.
    </p>


<h3>What is in the "go to build" email?</h3>
<ol>
<li> The exact code to be built from. Ideally, give the URL to the
explicit change in your source code repo that the release builds are
to be created from.
<ul><li>Comments like "use the latest code" are never ok: In one release,
after the "go to build" email was sent and before builds started, a
well intentioned developer landed a change without approval in the
wrong branch. The release included that unwanted change in the 
builds. Thankfully, this mistake was caught before we shipped, but we
did have to do a full stop and rebuild everything. 
</li>
<li>In a time-based Version Control System, like CVS, be fully
explicit of the exact time to use. Give time down to seconds, and
specify timezone. In one release, when Firefox was still based on CVS,
the Release Coordinator specified the cutoff time to be used for the
builds, but did not give the timezone. By the time Release Engineering
noticed the missing timezone info, the Release Coordinator was asleep
in his timezone. Release Engineering correctly guessed that the intent
was local time (in California), but in a late-night mixup over PDT
instead of PST, we ended up missing the last critical bug fix. This
was caught by QA before we shipped, but we did have to stop builds and
start build over using the correct cutoff time.
</li>
</ul>

<li>A clear sense of the urgency for this particular release.  This sounds so obvious, you
might be tempted to not bother including it. However, it is important
when handling some important edge cases, so here is a quick summary:
<ul> <li>Some releases are "routine", and can be worked on in normal
working hours. They are a pre-scheduled release, they are on schedule,
and there is no emergency. Of course, all release builds need to be
created in a timely manner, but there is no need for Release Engineers
to pull all-nighters and burn out humans for a "routine" release.
Instead, we schedule this out properly in advance, so everything stays
on schedule with people working normal hours. This keeps people fresh
and better able to handle unscheduled urgent work if the need
arises.</li>
	<li>Some releases are "urgent", where minutes matter. We
	call these "chemspills". These are typically to fix a published
	security exploit, or fix a newly-introduced-top-crash problem,
	impacting a large percentage of our userbase. These need to be
	created as quickly as possible and are typically not pre-scheduled
	releases.  </li>
	<li>Some releases change from "routine" to "chemspill" or from
	"chemspill" to "routine". For example, if a security fix in a
	"routine" release was accidentally leaked, it is now a "chemspill"
	release. If a business requirement like a "special sneak preview"
	release needed for an upcoming conference announcement was suddenly
	delayed for business reasons, the release now changes from
	"chemspill" to "routine".  </li>
	<li>Some releases have different people each holding different
	opinions on whether the release is "normal" or "urgent", depending
	on their perspective on the fixes being shipped in the release.</li>
	</ul> 
</li>
</ul>
<p>
  It is the role of the Release Coordinator to balance all the
	facts and opinions, reach a decision, and then communicate that
	decision about urgency consistently across all groups. If new
	information arrives, the Release Coordinator reassesses, and then
	communicates the new urgency to all the same groups. Having some
	groups believe a release is "urgent", while other groups believe the
	release is "normal" can be very destructive to cross-group cohesion
	across an organization.
</p> 
<p>Finally, these emails also became very useful to
	measure where time was spent during a release.  While these are only
	accurate to wall-clock time resolution, this accuracy is really
	helpful when figuring out where next to focus our efforts on making
	things faster.  As the old adage goes, before you can improve
	something, first you have to be able to measure it.
</p>
<p>
      Throughout the beta cycle for Firefox, we also do weekly
      releases from our <a
        href="http://hg.mozilla.org/releases/mozilla-beta/">mozilla-beta</a>
      repository. Each one of these beta releases goes through our
			usual full release automation and is treated almost identically
			to our regular final releases. To minimize surprises during a
			release, our intent is to have no new "untested" changes to
			release automation or infrastructure by the time we start the
			final release builds.
</p>

    </div>
    <div class="sect">
      <h2>6.2 Tagging, Building, and Source Tarballs</h2>
      <div class="figure" id="fig.ffreleng.tagging">
        <img src="../images/ffreleng/tagging.png" alt="Automated tagging" />
        <p>Figure&nbsp;6.2: Automated tagging</p>
      </div>
        <p>
        In preparation for starting automation, we recently started to
        use a script, <a
          href="http://mxr.mozilla.org/build/source/tools/buildbot-helpers/release_sanity.py">release_sanity.py</a>,
        to assist a Release Engineer with double-checking all
        configurations for a release match what is checked into our
				code, all properly tagged, and that our localization configs
				are also present and accounted for. This script also checks
				the build configuration files from nightly builds to release
				builds (where branding and update channels are set, among
        other things). If all the tests of this script pass, it will do
        a reconfig of the buildbot master where the release builders
        will run and then it generates the sendchange that triggers
        the automation.
      	</p>
        <p>
        After a Release Engineer passes release_sanity and kicks off
        builders, the first automated step in the Firefox release
        process is tagging all of the related source code repositories
        to record which revision of the source, language repos, and
        related tools are used for this version and build number of a
        release candidate. A single Firefox release uses code from
        about 85 version control repositories that host things such as
        the product code, localization strings, release automation
        code, and helper utilities. Tagging all these repositories is,
        therefore, critical to ensure that future steps of the release
        automation are all using a consistent set of revisions. It
        also has a number of other benefits: Linux distributions and
        other contributors can reproduce builds with exactly the same
        code that goes into the official builds, it also records the
        revisions of source and tools used on a per-release basis for
        future comparison of what changed between releases. For
				Firefox releases, one example tag is FIREFOX_6_0_2_RELEASE.
        </p>
        <p>
				Once all the repositories are tagged, a series of dependent
				builders automatically start up: one builder for each release
				platform plus a source bundle that includes all source used in
				the release.  Source bundle and built installers are all
				uploaded to the release directory as they become available.
				This allows anyone to see exactly what code is in a release
				and gives a snapshot that would allow us to re-create the
				builds if we ever needed to (if our VCS failed somehow).
        </p>
        <p>
        For the main Firefox source repository, the first thing we
        actually do is to create a release branch (relbranch) based on
        the signed-off revision given by the Release Coordinator.
        This release branch is implemented as an in-repository named
        branch whose parent changeset is the signed-off revision. The
        release branch is used to make release-specific modifications
        to the source code, such as bumping the version numbers, or
        finalizing the set of locales that will be built. If a
        critical security vulnerability is discovered in the future
        that requires an immediate fix (a "chemspill" situation), a
        minimal set of changes to address the vulnerability will be
        landed on this relbranch and a new version of Firefox
        released. When we have to do another round of builds for a
        particular release, buildN,  we use these relbranches to grab
        the same code that was signed off on for 'go to build' which
        is where any changes to that release code will have been
        landed. The automation starts again and bumps the tagging to
        the new changeset on that relbranch.  
				</p>
        <p>
        Our tagging process does a <em>lot</em> of operations with
        local and remote Mercurial (hg) repositories. To factor out
        some of the most common operations we've written a few tools
        to assist us: <em><a
            href="http://hg.mozilla.org/build/mozharness/file/a0fce0162fd5/scripts/hgtool.py">hgtool.py</a></em>
        and <em><a
            href="http://hg.mozilla.org/build/tools/file/7adc08bd1386/lib/python/util/retry.py">retry.py</a></em>.
        </p> <p> <em><a
        href="http://hg.mozilla.org/build/tools/file/7adc08bd1386/lib/python/util/retry.py">retry.py</a></em>
        is a simple wrapper that can take a given command and run it,
        retrying several times if it fails. It can also watch for
        exceptional output conditions and retry or report failure in those
        cases. We've found it useful to wrap retry.py around most of the
        commands which can fail due to external dependencies.  For
        tagging, the hg operations could fail due to temporary network
        outages, web server issues, or the backend hg server being
        temporarily overloaded. Being able to automatically retry these
        operations and continue on saves a lot of our time, since we don't
        have to manually recover and get the release automation running
        again.
        </p>
        <p>
        <em><a
            href="http://hg.mozilla.org/build/mozharness/file/a0fce0162fd5/scripts/hgtool.py">hgtool.py</a></em>
        is a utility that encapsulates several common hg operations,
        like cloning/pulling/updating with a single invocation. It
        also adds support for hg's share extension, which we use
        extensively to avoid having to have several full clones of
        repositories in different directories on the same machine.
        Adding support for shared local repositories significant
        sped up our tagging process since most full clones of the
        product and locale repositories could be avoided.
        </p>
        <p>
        An important consideration for factoring out tools like these
        is to make our automation as testable as possible. Because
        tools like <em><a href="http://hg.mozilla.org/build/mozharness/file/a0fce0162fd5/scripts/hgtool.py">hgtool.py</a></em> are small, single purpose utilities built
        on top of reusable libraries, they're much easier to test in
        isolation. 
        </p>
        <p>
        Today our tagging is done in two parallel jobs: one for
        desktop Firefox which takes around 20 minutes to complete,
        and another for mobile Firefox which takes around 10 minutes
        to complete. In the future we would like to streamline our release
        automation process so that we tag <em>all</em> the various repositories
        in parallel. The initial builds can be started as soon as the
				product code  and tools talos requirement repository is tagged
				without having to wait for all the locale repositories to be
				tagged. By the time these builds are finished, the rest of the
				repositories will have been tagged so that localization
				repackages and future steps can be completed.
        We estimate this can reduce the total time to have builds
        ready by 15 minutes.
        </p>
    </div>
    <div class="sect">
      <h2>6.3 Localization Repacks and Partner Repacks</h2>
      <div class="figure" id="fig.ffreleng.repacks_l10n">
        <img src="../images/ffreleng/repacks_l10n.png" alt="Repacking Firefox for each localization" />
        <p>Figure&nbsp;6.3: Repacking Firefox for each localization</p>
      </div>
        <p>Once the desktop builds are generated and uploaded to FTP, our
        automation triggers the localization repackaging jobs. This consists of a
        handful of jobs that take the original build (using the en-US locale),
        unpacking it and stuffing in the strings for each locale that we are shipping
        for this release (this is why we call them repackages). Each job takes a
        handful of locales. This approach allows us to parallelize the jobs
        across many machines. More explicitly, instead of doing all 84
        localizations  in one machine, we can split them across
        six different machines and take approximately a sixth of the time it
        would have required to do so in one machine. We could split it
        even further but we would take away too many machines from the
        pool which would affect jobs triggered by our developers as part of
        our contiuous CI system. 
        </p>
        <p>
        The process for mobile (Android) is slightly different as we
        produce only two installers: an English version and a
        multi-language version with just a dozen of languages built
        into the installer instead of a build per language. In the
        future, other languages will be requested on demand as add-ons
        from addons.mozilla.org.
        </p>
        <p>
        In Figure 6.3 you can see that we currently rely on three
        different sources for our locale information. There is a plan
        to move all three into a unified json file.  These files
        contain information about the different localizations we have
        and certain platform exceptions.  Specifically, for a given
        localization we need to know which revision of the repository
        to use for a given release and we need to know if the
        localization can build on all of our supported platforms (e.g.
        Japanese for Mac comes from a different repo all together).
        Two of these files are used for the Desktop releases and one
        for the Mobile release (this json file contains both the list
        of platforms and the changesets). 
        </p>
        <p>
        Who decides which languages we ship? First of all, localizers
        themselves nominate their specific changeset for a given
        release. The nominated changeset gets reviewed by Mozilla's
        localization team and shows up in a web dashboard that lists
        the changesets needed for each language. On the day of a
        release we retrieve this list of changesets and we repackage
        them accordingly.
        </p>
        <p>
        Besides localization repackages we also generate partner
        repackages. These are customized builds for various partners
        we have who want to customize the experience for their
        customers.  The main type of changes are custom bookmarks,
        custom homepage and custom search engines but many other
        things can be changed. These customized builds are generated
        for the latest Firefox release and not for betas.
        </p>
    </div>
    <div class="sect">
      <h2>6.4 Signing</h2>
      <div class="figure" id="fig.ffreleng.signing">
        <img src="../images/ffreleng/signing.png" alt="Signing Firefox installers" />
        <p>Figure&nbsp;6.4: Signing of Firefox installers, partially automated</p>
      </div>
      <p>
      Until recently doing so involved a release engineer on a signing
      master for almost an hour doing the steps of downloading builds,
      signing them, and uploading them back to FTP before the
      automation could continue. Once signing on the master is
      completed and all files have been uploaded a win32_signing.log
      file is uploaded to the release candidates directory.  This
      signifies the end of human signing and dependent builders
      watching for that file continue automation from that point on.
      Recently though we've added an additional wrapper of automation
      around the signing steps. The release engineer merely starts an
      automated signing process on the signing master then goes off to
      do other work while a simple set makefile targets does the heavy
      lifting.
      </p>
      <p>
      The engineer will open a Cygwin shell on the signing master and
      set up a few environment variables pertaining to the release
      like VERSION, BUILD, TAG, RELEASE_CONFIG that help the script
      find the right directories on FTP and know when all the
      deliverables for a release have been downloaded so that the
      signing can start. After checking out the most recent version of
      our signing tools they simply do a 'make autosign'. The engineer
      will need to manually enter 2 passphrases, gpg and signcode
      which are verified by the make scripts then it starts a download
      loop that watches for uploaded builds &amp; repacks from the release
      automation.  Once the last item has showed up (all items
      progressively downloaded as available) signing starts right away
      which is a great improvement as this can happen during non-work
      hours and provide our QA team with signed installers to test
      promptly without waiting for an engineer to come on shift. The
      win32 installers are signed with Authenticode and then we
      generate gpg signatures for the other platforms, all
      deliverables will have an MD5SUM and SHA1SUM generated for them
      and those hash values are written to files of the same name.
      These files will be uploaded back to the release-candidates
      directory as well as synced into the final home of a release
      once it is live so that anyone who downloads a Firefox installer
      from our mirrors can ensure they got the correct object. When
      all signed bits are available and verified (we do a quick and
      more in-depth verification process) they are uploaded back to
      FTP along with the signing log file that the automation is
      waiting for to proceed.
      </p>
      <p>
      Our next round of improvements to the signing process will be
      creating a tool that allows us to sign bits at the time of
      build/repack. This work involves creating a signing server
      application that will receive requests for signing files from
      the release build machines. Also required is a signing client
      tool, which contacts the signing server, authenticates itself as
      a machine trusted to request signing, uploads the files to be
      signed, downloads the signed bits, and then includes them as
      part of the packaged build. Doing signing as part of the build
      process really streamlines our process, as it allows us to skip
      over the expensive all-at-once signing process described above,
      as well as the expensive generate-all-updates process described
      below.
      </p>
    </div>
    <div class="sect">
      <h2>6.5 Updates</h2>
      <p>
      We generate a LOT of updates.  Every platform, every locale,
      every installer from Firefox N -> Firefox NOW in both complete
      and partial forms.  Depending on the branch, we generate updates
      for all supported previous releases from that branch. Our
      automation bumps the update configuration files of each
      release's build off a branch to maintain our canonical list of
      what version numbers, platforms, and localizations need to have
      updates created to offer users this newest release. We offer
      updates as 'snippets' which you'll see an example of below,
      simply an xml pointer file hosted on our AUS (Application Update
      Service) that informs the user's Firefox of where complete
      and/or partial .mar (Mozilla Archive) files are hosted.
      </p>
      <h4>Major vs. Minor</h4>
      <p>
      As you'll see in the snippet sample below, we have a type
      setting for updates. Most updates fall into the "minor" category
      which means that, for the user, the downloading of the partial
      update package (though sometimes it's the complete when that is
      the only update available for a particular version/platform)
      happens quietly in the background and when it's ready to be
      applied the user is informed of this and given the option to
      apply &amp; restart.  Minor updates are used to keep people up to
      date within their channel: beta releases update to the next beta
      release, nightlies to the following night's build.  Major
      updates have been used when we needed to advertise to our users
      that the latest &amp; greatest release was available and to prompt
      the user letting them know "A new version of Firefox is
      available, would you like to update?". Our new rapid-release
      versions means no longer needing to do as many major updates,
      we'll be able to stop generating them once 3.6.* is no longer
      supported.
      </p>
      <h4>Sample Update Snippet</h4>
      <code class="class-name">
      &lt;updates&gt;<br />
      &nbsp;&lt;update type="minor"  version="7.0.1" extensionVersion="7.0.1" buildID="20110928134238" detailsURL="https://www.mozilla.com/en-US/firefox/7.0.1/releasenotes/"&gt;<br />
      &nbsp;&nbsp;&lt;patch type="complete" URL="http://download.mozilla.org/?product=firefox-7.0.1-complete&os=osx&lang=en-US&force=1" hashFunction="SHA512" hashValue="7ecdbc110468b9b4627299794d793874436353dc36c80151550b08830f9d8c5afd7940c51df9270d54e11fd99806f41368c0f88721fa17e01ea959144f473f9d" size="28680122"/&gt;<br />
      &nbsp;&nbsp;&lt;patch type="partial" URL="http://download.mozilla.org/?product=firefox-7.0.1-partial-6.0.2&os=osx&lang=en-US&force=1" hashFunction="SHA512" hashValue="e9bb49bee862c7a8000de6508d006edf29778b5dbede4deaf3cfa05c22521fc775da126f5057621960d327615b5186b27d75a378b00981394716e93fc5cca11a" size="10469801"/&gt;<br />
      &nbsp;&lt;/update&gt;<br />
      &lt;/updates&gt;
      </code>
      <h4>What's in an update?</h4>
      <p>
      At build time we generate complete mars which contain all the
      bits for the new release compressed with bz2 and then archived
      in a mar file. Complete updates are created so users can be
      auto-updated using our updater binary instead of having to point
      to (and having the user download) a standalone installer. Both
      complete and partial updates are downloaded automatically
      through the update channel a user is on. Partial update mars are
      created by doing a binary diff of the old version's complete mar
      to the new version's and creating the mar with the diff and a
      manifest file. As you can see in the sample snippet above, this
      results in a much smaller file size for partial updates. In
      older versions of our update automation the generation of
      partial updates could take 6-7 hours as each locale, for each
      platform had the complete mars downloaded, diffed, then packed
      up for a partial update.  Eventually it was discovered that even
      across platforms, many component changes were identical and so
      with a script that cached the hash for each part of the diff our
      partial update creation time was brought down to ~40 minutes
      since so many changes could be re-used. After the snippets have
      been uploaded and are hosted on AUS, an update verification step
      is run to a) test downloading the snippets and b) run the
      updater with the downloaded mar to confirm that the updates
      apply correctly.
      </p>
      <p>
      Generation of partial update mars as well as all the update
      snippets is currently done after signing is complete. We do this
      because generation of the partial updates must be done between
      two signed versions of the files, and therefore generation of
      the snippets must also wait until the partial mars are
      available. Once we're able to integrate signing into the build
      process, we can generate partial updates immediately after
      completing a build or repack. Together with improvements to our
      AUS software, this effectively means that once we're finished
      builds and repacks we can push immediately to mirrors. This
      effectively parallelizes the creation of all the updates,
      removing several hours from our total time.
      </p>
    </div>
    
    <div class="sect">
      <h2>6.6 Pushing to Internal Mirrors &amp; QA</h2>
      <p>Verifying that the release process is producing the expected
      deliverables is key for producing the right bits for our users. This
      is accomplished by QA's verification and sign offs process along the
      way to ensure that everything is going according to the plan.
      </p>
      
      <p>
			QA does manual and automated testing of the builds as soon as
			they become available. QA relies on people in different
			timezones, both community and contractors, to speed up this
			validation process. Meanwhile, the release automation generates
			updates for all languages and all platforms. Once these are
			ready, QA tests that users would be able to migrate from the
			previous release to the current one using these updates. 
      </p>
			<p>We also push the binaries to our internal mirrors (mozilla
			hosted storage, later we make this available to our community
			mirrors for wider availability) to help us handle the load of
			users requesting their updates rather than reaching ftp
			directly. In the case of betas this is around of one million
			users. Notice that users don't get the updates until QA has
			signed off and the Release Coordinator has sent the email asking
			to push them live.
      </p>
      <p>
      The validation process after builds and updates are generated is:
      <ol>
         <li>QA, along with community and contractors in other
				 timezones, do manual testing.</li>
				 <li>QA triggers the automation systems to do functional
				 testing.</li> 
				 <li>QA verifies that fixed problems for that
				 release are indeed fixed.</li>
				 <li>The release automation meanwhile is generating the
				 updates.</li>
         <li>QA signs off the builds.</li>
         <li>QA signs off the updates.</li>
      </ol>
      </p>
      <p>At this point we are ready to go live which is covered in the
      following section.
      </p>
    </div>
    <div class="sect">
      <h2>6.7 Pushing to Public Mirrors and AUS</h2>
      <!-- TODO  write about Bouncer? -->
      <p>
      Pushing this latest Firefox release public is then pretty
      straightforward. The Release Coordinator gives the go ahead to have
      the files pushed to our community mirror network. We rely on our
      community mirrors to be able to handle a few hundred million
      users downloading updates over the next few days. All the
      installers, complete and partial updates for all platforms and
      locales are already on our internal mirror network at this
      point. Publishing the files to our external mirrors involves
      changes to an rsync exclude file for the public mirrors module.
      Once this change is made, the mirrors will start to synchronize
      the new release files. Each mirror has a score or weighting
      associated with it, and we monitor which mirrors have
      synchronized the files and sum their individual scores to
      compute a total "uptake" score. Once a certain uptake is
      reached, we notify the Release Coordinator that the mirrors have
      enough uptake to handle the release.
      </p>
      <p>
      This is the point at which the release becomes "official". The
      Release Coordinator sends the final "go", and we update some symlinks
      on the web server so that visitors to our web and ftp sites can
      find the latest version. We also publish all the update snippets
      for past versions of Firefox to our AUS (automated update
      service) system. Firefox on users' machines regularly checks the
      AUS servers to see if there's an updated version of Firefox
      available for them. Once we publish these update snippets, users
      are able to automatically update Firefox on their machines to
      the latest version.
      </p>
    </div>
  <div class="sect">
      <h2>6.8 Working in the open</h2>
      <p>
			At Mozilla we strive to do everything in the open. The Firefox
			developers make all their code changes in the open. Similarly,
			all of Release Engineering's code and blow-by-blow build notes
			are public, available for everyone to see.  Here are some links
			for further reading:
      </p>
      <ul>

      <li><a href="http://atlee.ca/blog">http://atlee.ca/blog</a>
      <li><a href="http://crashopensource.blogspot.com">http://crashopensource.blogspot.com</a>
      <li><a href="http://oduinn.com">http://oduinn.com</a>
      <li><a href="http://armenzg.blogspot.com">http://armenzg.blogspot.com</a>
      <li><a href="https://wiki.mozilla.org/Release:Release_Automation_on_Mercurial:Documentation">https://wiki.mozilla.org/Release:Release_Automation_on_Mercurial:Documentation</a> Documentation on design and flow of our Mercurial-based release process.</li>
			<li><a
			href="http://hg.mozilla.org/build">http://hg.mozilla.org/build<a>
			Release Engineering's build repositories. In particular, the
			buildbotcustom, buildbot-configs, and tools repositories are
			used heavily for releases.</li> 
			<li><a
			href="https://wiki.mozilla.org/Releases/Firefox_7.0b4/BuildNotes">https://wiki.mozilla.org/Releases/Firefox_7.0b4/BuildNotes</a>
			The Firefox 7.0 Beta 4 Build Notes.  In addition to code, we
			document every aspect of a release. Here's an example from our
			7.0b4 release, but you can find all our release notes if you
			edit the URL appropriately.</li> 
			</ul>
      <p>
			In the last few years, we've made a lot of changes to our
			automation.  These changes have made a material difference to
			Mozilla's ability to deliver "new rapid release cadence"
			releases, as well as "urgent security fixes" releases to Firefox
			users.  With this chapter, we've tried to give a quick overview
			of the things that did, and did not, work for us. Hopefully,
			people find this information useful. If you have
			comments/feedback, or even better details on things you've done
			differently that did/didnt work better for you, please contact
			us.  We'd love to hear from you.
			</p>
    </div>
    <div class="footer">
    </div>
  </body>
</html>
